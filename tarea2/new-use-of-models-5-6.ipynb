{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "34ef8e0c",
      "metadata": {
        "id": "34ef8e0c"
      },
      "source": [
        "(15p) Using the test dataset, calculate the perplexity of each language model. Report the results obtained. If you experience variable overflow, use probabilities in log space."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75d4241d",
      "metadata": {
        "id": "75d4241d"
      },
      "source": [
        "El link de la carpeta donde estan los conjuntos de entrenamiento + testing y los modelos entrenados es:\n",
        "\n",
        "https://uniandes-my.sharepoint.com/:f:/g/personal/a_mosquerah2_uniandes_edu_co/Em-od1gldI9BnnTjpUXqZXcB8fjXUoybI35zktWPWzyqpw?e=6dT4ng\n",
        "\n",
        "(Solo se puede abrir con correo uniandes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6628e25b",
      "metadata": {
        "id": "6628e25b"
      },
      "outputs": [],
      "source": [
        "# carpeta conjunto de entrenamiento y de testing de 20news y de BAC\n",
        "tercer_punto_folder: str = \"tercer-punto\"\n",
        "# carpeta modelos de unigramas, bigramas y trigramas de cada corpus.\n",
        "cuarto_punto_folder: str = \"cuarto-punto\"\n",
        "group_code: str = \"group-ansada\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6c6583ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c6583ef",
        "outputId": "7f400d43-3e0b-4ba4-9c81-ff2c3bc5f6b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculando perplejidad (RAM<10GB, VRAM≈0, trigram en SQLite) ---\n",
            "[OK] Modelo 1-gramas cargado: 20N_group-ansada_unigrams.json.gz\n",
            "[OK] Modelo 2-gramas cargado: 20N_group-ansada_bigrams.json.gz\n",
            "[OK] Trigramas indexados en cuarto-punto/models/20N_group-ansada_trigrams.sqlite\n",
            "\n",
            "=== Perplejidad 20N ===\n",
            "Unigramas : 1105.8665\n",
            "Bigramas  : 1898.1203\n",
            "Trigramas : 4892.7925\n",
            "[OK] Modelo 1-gramas cargado: BAC_group-ansada_unigrams.json.gz\n",
            "[OK] Modelo 2-gramas cargado: BAC_group-ansada_bigrams.json.gz\n",
            "[OK] Trigramas indexados en cuarto-punto/models/BAC_group-ansada_trigrams.sqlite\n",
            "\n",
            "=== Perplejidad BAC ===\n",
            "Unigramas : 827.3796\n",
            "Bigramas  : 1038.3964\n",
            "Trigramas : 5444.9151\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Perplejidad con backoff y trigramas en DISCO (SQLite) — Tipado + Docstrings\n",
        "# =========================\n",
        "import os\n",
        "import re\n",
        "import gzip\n",
        "import json\n",
        "import math\n",
        "import sqlite3\n",
        "from functools import lru_cache\n",
        "from typing import Dict, Optional, Tuple, Callable, Iterator\n",
        "\n",
        "\n",
        "def _json_load_any(path: str) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Carga un diccionario JSON {clave: prob} desde .json o .json.gz sin materializar más de lo necesario.\n",
        "\n",
        "    Args:\n",
        "        path: Ruta al archivo .json o .json.gz.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: Mapa clave→probabilidad (por ejemplo, \"w1 w2\": 0.00123).\n",
        "    \"\"\"\n",
        "    if path.endswith(\".gz\"):\n",
        "        with gzip.open(path, \"rt\", encoding=\"utf-8\") as f:  # type: ignore[name-defined]\n",
        "            return json.load(f)\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def read_ngram_model_smart(prefix: str, n: int) -> Optional[Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Localiza y carga el modelo n-grama del corpus dado.\n",
        "\n",
        "    Convención de nombres:\n",
        "      n=1 -> *_unigrams.json(.gz)\n",
        "      n=2 -> *_bigrams.json(.gz)\n",
        "      n=3 -> *_trigrams.json(.gz)\n",
        "\n",
        "    Args:\n",
        "        prefix: \"20N\" o \"BAC\".\n",
        "        n: Orden del modelo (1, 2, 3).\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float] si existe; None en caso contrario.\n",
        "    \"\"\"\n",
        "    suffix: str = {1: \"uni\", 2: \"bi\", 3: \"tri\"}[n]\n",
        "    base: str = os.path.join(cuarto_punto_folder, \"models\", f\"{prefix}_{group_code}\")\n",
        "    candidates = [f\"{base}_{suffix}grams.json\", f\"{base}_{suffix}grams.json.gz\"]\n",
        "    for p in candidates:\n",
        "        if os.path.exists(p):\n",
        "            try:\n",
        "                model = _json_load_any(p)\n",
        "                print(f\"[OK] Modelo {n}-gramas cargado: {os.path.basename(p)}\")\n",
        "                return model\n",
        "            except Exception as e:  # pragma: no cover (logging)\n",
        "                print(f\"[WARN] Falló carga de {p}: {e}\")\n",
        "    print(f\"[ERROR] No se encontró modelo {n}-gramas para {prefix}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def _trigram_json_gz_to_sqlite(gz_path: str, sqlite_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Convierte un JSON(.gz) de trigramas {\"w1 w2 w3\": p, ...} a una tabla SQLite:\n",
        "        tri(w1 TEXT, w2 TEXT, w3 TEXT, p REAL, PRIMARY KEY(w1,w2,w3))\n",
        "\n",
        "    Se procesa en streaming para evitar usar RAM alta.\n",
        "\n",
        "    Args:\n",
        "        gz_path: Ruta al .json.gz de trigramas.\n",
        "        sqlite_path: Ruta de salida .sqlite.\n",
        "    \"\"\"\n",
        "    if os.path.exists(sqlite_path):\n",
        "        return\n",
        "    os.makedirs(os.path.dirname(sqlite_path), exist_ok=True)\n",
        "    conn = sqlite3.connect(sqlite_path)\n",
        "    cur = conn.cursor()\n",
        "    cur.executescript(\n",
        "        \"PRAGMA journal_mode=OFF; PRAGMA synchronous=OFF; PRAGMA temp_store=MEMORY;\"\n",
        "        \"CREATE TABLE tri (w1 TEXT, w2 TEXT, w3 TEXT, p REAL, PRIMARY KEY(w1,w2,w3));\"\n",
        "    )\n",
        "\n",
        "    pat: re.Pattern[str] = re.compile(r'^\\s*\"([^\"]+)\":\\s*([0-9eE+\\-\\.]+)\\s*,?\\s*$')\n",
        "    batch: list[tuple[str, str, str, float]] = []\n",
        "\n",
        "    with gzip.open(gz_path, \"rt\", encoding=\"utf-8\") as f:  # type: ignore[name-defined]\n",
        "        _ = f.readline()  # '{'\n",
        "        for line in f:\n",
        "            s = line.strip()\n",
        "            if s == \"}\":\n",
        "                break\n",
        "            m = pat.match(s)\n",
        "            if not m:\n",
        "                continue\n",
        "            key, p_str = m.group(1), m.group(2)\n",
        "            parts = key.split(\" \")\n",
        "            if len(parts) != 3:\n",
        "                continue\n",
        "            batch.append((parts[0], parts[1], parts[2], float(p_str)))\n",
        "            if len(batch) >= 50_000:\n",
        "                cur.executemany(\"INSERT OR REPLACE INTO tri(w1,w2,w3,p) VALUES(?,?,?,?)\", batch)\n",
        "                conn.commit()\n",
        "                batch.clear()\n",
        "    if batch:\n",
        "        cur.executemany(\"INSERT OR REPLACE INTO tri(w1,w2,w3,p) VALUES(?,?,?,?)\", batch)\n",
        "        conn.commit()\n",
        "    cur.execute(\"ANALYZE\")\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(f\"[OK] Trigramas indexados en {sqlite_path}\")\n",
        "\n",
        "\n",
        "def _make_tri_getter(prefix: str) -> Tuple[Callable[[str, str, str], Optional[float]], sqlite3.Connection]:\n",
        "    \"\"\"\n",
        "    Prepara el acceso a trigramas vía SQLite y devuelve:\n",
        "      - una función tri_get(w1, w2, w3) -> prob o None\n",
        "      - la conexión SQLite (para cerrar al terminar)\n",
        "\n",
        "    Args:\n",
        "        prefix: \"20N\" o \"BAC\".\n",
        "\n",
        "    Returns:\n",
        "        (tri_get, conn)\n",
        "    \"\"\"\n",
        "    gz = os.path.join(cuarto_punto_folder, \"models\", f\"{prefix}_{group_code}_trigrams.json.gz\")\n",
        "    db = os.path.join(cuarto_punto_folder, \"models\", f\"{prefix}_{group_code}_trigrams.sqlite\")\n",
        "    if not os.path.exists(gz):\n",
        "        gz_alt = gz[:-3]\n",
        "        if os.path.exists(gz_alt):\n",
        "            raise RuntimeError(\"Convierte el .json de trigramas a .json.gz o ajusta el parser.\")\n",
        "        raise FileNotFoundError(f\"No existe {gz} ni {gz_alt}\")\n",
        "    _trigram_json_gz_to_sqlite(gz, db)\n",
        "\n",
        "    conn = sqlite3.connect(db)\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    @lru_cache(maxsize=200_000)\n",
        "    def tri_get(a: str, b: str, c: str) -> Optional[float]:\n",
        "        row = cur.execute(\"SELECT p FROM tri WHERE w1=? AND w2=? AND w3=?\", (a, b, c)).fetchone()\n",
        "        return float(row[0]) if row else None\n",
        "\n",
        "    return tri_get, conn\n",
        "\n",
        "\n",
        "def calculate_perplexity_backoff_sqltri(\n",
        "    testing_file: str,\n",
        "    uni: Dict[str, float],\n",
        "    bi: Dict[str, float],\n",
        "    tri_get: Callable[[str, str, str], Optional[float]]\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Calcula perplejidad de modelos unigrama/bigrama/trigrama sobre un corpus de prueba.\n",
        "\n",
        "    - Unigrama: usa p_unigram(w). Si falta, usa epsilon.\n",
        "    - Bigrama : P(w1) * Π P(w_i | w_{i-1}); si (w_{i-1}, w_i) falta, backoff a p_unigram(w_i).\n",
        "    - Trigrama: P(w1) * P(w2|w1) * Π P(w_i | w_{i-2}, w_{i-1});\n",
        "                si tri falta -> backoff a bi; si bi falta -> backoff a uni.\n",
        "\n",
        "    Se acumulan log-probabilidades con *clipping* para evitar underflow.\n",
        "\n",
        "    Args:\n",
        "        testing_file: Ruta del archivo de prueba (una oración por línea).\n",
        "        uni: Diccionario unigrama {\"w\": p}.\n",
        "        bi: Diccionario bigrama {\"w1 w2\": p}.\n",
        "        tri_get: Función que consulta trigramas en disco.\n",
        "\n",
        "    Returns:\n",
        "        (ppl_unigram, ppl_bigram, ppl_trigram)\n",
        "    \"\"\"\n",
        "    epsilon_uni: float = 1.0 / max(1, 10 * len(uni))  # piso seguro OOV\n",
        "    total_log_u: float = 0.0\n",
        "    total_log_b: float = 0.0\n",
        "    total_log_t: float = 0.0\n",
        "    total_tokens: int = 0\n",
        "\n",
        "    def _log(p: float) -> float:\n",
        "        return math.log(p if p > 1e-300 else 1e-300)\n",
        "\n",
        "    with open(testing_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        for raw in f:\n",
        "            s = raw.strip()\n",
        "            if not s:\n",
        "                continue\n",
        "            toks = s.split()\n",
        "            n = len(toks)\n",
        "            if n == 0:\n",
        "                continue\n",
        "\n",
        "            total_tokens += n\n",
        "\n",
        "            # --- Unigram ---\n",
        "            for w in toks:\n",
        "                p = uni.get(w, epsilon_uni)\n",
        "                total_log_u += _log(p)\n",
        "\n",
        "            # --- Bigram ---\n",
        "            p1 = uni.get(toks[0], epsilon_uni)\n",
        "            total_log_b += _log(p1)\n",
        "            for i in range(n - 1):\n",
        "                a, b_ = toks[i], toks[i + 1]\n",
        "                p_bi = bi.get(f\"{a} {b_}\")\n",
        "                if p_bi is None:\n",
        "                    p_bi = uni.get(b_, epsilon_uni)\n",
        "                total_log_b += _log(p_bi)\n",
        "\n",
        "            # --- Trigram ---\n",
        "            p1 = uni.get(toks[0], epsilon_uni)\n",
        "            total_log_t += _log(p1)\n",
        "            if n >= 2:\n",
        "                p2 = bi.get(f\"{toks[0]} {toks[1]}\")\n",
        "                if p2 is None:\n",
        "                    p2 = uni.get(toks[1], epsilon_uni)\n",
        "                total_log_t += _log(p2)\n",
        "            for i in range(n - 2):\n",
        "                a, b_, c = toks[i], toks[i + 1], toks[i + 2]\n",
        "                p_tri = tri_get(a, b_, c)\n",
        "                if p_tri is None:\n",
        "                    p_tri = bi.get(f\"{b_} {c}\")\n",
        "                    if p_tri is None:\n",
        "                        p_tri = uni.get(c, epsilon_uni)\n",
        "                total_log_t += _log(p_tri)\n",
        "\n",
        "    if total_tokens == 0:\n",
        "        return float(\"inf\"), float(\"inf\"), float(\"inf\")\n",
        "\n",
        "    ppl_u: float = math.exp(-total_log_u / total_tokens)\n",
        "    ppl_b: float = math.exp(-total_log_b / total_tokens)\n",
        "    ppl_t: float = math.exp(-total_log_t / total_tokens)\n",
        "    return ppl_u, ppl_b, ppl_t\n",
        "\n",
        "\n",
        "def run_perplexity_for(prefix: str) -> Optional[Tuple[float, float, float]]:\n",
        "    \"\"\"\n",
        "    Ejecuta el cálculo de perplejidad para un corpus ('20N' o 'BAC'):\n",
        "      - Carga uni/bi en RAM.\n",
        "      - Abre acceso a tri en SQLite.\n",
        "      - Evalúa sobre el archivo de testing de 'tercer-punto/'.\n",
        "\n",
        "    Args:\n",
        "        prefix: \"20N\" o \"BAC\".\n",
        "\n",
        "    Returns:\n",
        "        (ppl_uni, ppl_bi, ppl_tri) o None si faltan modelos.\n",
        "    \"\"\"\n",
        "    testing_file: str = os.path.join(tercer_punto_folder, f\"{prefix}_{group_code}_testing.txt\")\n",
        "\n",
        "    uni_raw = read_ngram_model_smart(prefix, 1)\n",
        "    bi_raw = read_ngram_model_smart(prefix, 2)\n",
        "    if not all([uni_raw, bi_raw]):\n",
        "        print(f\"[SKIP] Faltan uni/bi para {prefix}\")\n",
        "        return None\n",
        "\n",
        "    # Llaves en RAM tal como están en archivo (sin duplicar estructuras)\n",
        "    uni: Dict[str, float] = {k: float(v) for k, v in uni_raw.items()}\n",
        "    bi: Dict[str, float] = {k: float(v) for k, v in bi_raw.items()}\n",
        "\n",
        "    tri_get, conn = _make_tri_getter(prefix)\n",
        "    try:\n",
        "        pu, pb, pt = calculate_perplexity_backoff_sqltri(testing_file, uni, bi, tri_get)\n",
        "        print(f\"\\n=== Perplejidad {prefix} ===\")\n",
        "        print(f\"Unigramas : {pu:.4f}\")\n",
        "        print(f\"Bigramas  : {pb:.4f}\")\n",
        "        print(f\"Trigramas : {pt:.4f}\")\n",
        "        return pu, pb, pt\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "\n",
        "def run_all_and_report(save_path: Optional[str] = None) -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Ejecuta perplejidad para ambos corpus (20N/BAC) y devuelve (y opcionalmente guarda) un reporte.\n",
        "\n",
        "    Args:\n",
        "        save_path: Ruta JSON para guardar resultados; si None, no guarda.\n",
        "\n",
        "    Returns:\n",
        "        Dict con estructura:\n",
        "        {\n",
        "          \"20N\": {\"unigram\": ..., \"bigram\": ..., \"trigram\": ...},\n",
        "          \"BAC\": {\"unigram\": ..., \"bigram\": ..., \"trigram\": ...}\n",
        "        }\n",
        "    \"\"\"\n",
        "    report: Dict[str, Dict[str, float]] = {}\n",
        "    for prefix in (\"20N\", \"BAC\"):\n",
        "        res = run_perplexity_for(prefix)\n",
        "        if res is not None:\n",
        "            pu, pb, pt = res\n",
        "            report[prefix] = {\"unigram\": pu, \"bigram\": pb, \"trigram\": pt}\n",
        "    if save_path:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(report, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"[OK] Reporte guardado en {save_path}\")\n",
        "    return report\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\n--- Calculando perplejidad (RAM<10GB, VRAM≈0, trigram en SQLite) ---\")\n",
        "    run_perplexity_for(\"20N\")\n",
        "    run_perplexity_for(\"BAC\")\n",
        "    # Opcional: guardar reporte\n",
        "    # run_all_and_report(os.path.join(cuarto_punto_folder, \"perplexity_report.json\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e150c750",
      "metadata": {
        "id": "e150c750"
      },
      "source": [
        "(15p) Using your best language model, build a method/function that automatically generates sentences by receiving the first word of a sentence as input. Take different tests and document them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "df715e92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df715e92",
        "outputId": "158e8308-1976-4456-a48f-d3372a9ad628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Samples 20N ---\n",
            "[20N] w='the' cfg={'order': 3, 'temperature': 0.7, 'top_k': 60, 'top_p': 0.95, 'seed': 7} -> the NUM and NUM\n",
            "[20N] w='this' cfg={'order': 3, 'temperature': 0.7, 'top_k': 60, 'top_p': 0.95, 'seed': 7} -> this is a matter of have you ever hated someone\n",
            "[20N] w='i' cfg={'order': 3, 'temperature': 0.7, 'top_k': 60, 'top_p': 0.95, 'seed': 7} -> i dont know about the clipper chip and other <UNK> assholes just move closer\n",
            "[20N] w='if' cfg={'order': 3, 'temperature': 0.7, 'top_k': 60, 'top_p': 0.95, 'seed': 7} -> if you are\n",
            "[20N] w='the' cfg={'order': 3, 'temperature': 1.0, 'top_k': 60, 'top_p': 0.95, 'seed': 21} -> the first three decades before embarking on a good life\n",
            "[20N] w='this' cfg={'order': 3, 'temperature': 1.0, 'top_k': 60, 'top_p': 0.95, 'seed': 21} -> this is that he would have an argument is invalid because it was NUM NUM NUM NUM NUM NUM a clone of mswindows\n",
            "[20N] w='i' cfg={'order': 3, 'temperature': 1.0, 'top_k': 60, 'top_p': 0.95, 'seed': 21} -> i dont believe what i was <UNK> hell followed with a NUM meg hd\n",
            "[20N] w='if' cfg={'order': 3, 'temperature': 1.0, 'top_k': 60, 'top_p': 0.95, 'seed': 21} -> if you think they have a real fast you drive\n",
            "[20N] w='the' cfg={'order': 1, 'temperature': 1.0, 'top_k': 0, 'top_p': 1.0, 'seed': 7} -> the for NUM war those have also\n",
            "[20N] w='this' cfg={'order': 1, 'temperature': 1.0, 'top_k': 0, 'top_p': 1.0, 'seed': 7} -> this for NUM war those have also\n",
            "[20N] w='i' cfg={'order': 1, 'temperature': 1.0, 'top_k': 0, 'top_p': 1.0, 'seed': 7} -> i for NUM war those have also\n",
            "[20N] w='if' cfg={'order': 1, 'temperature': 1.0, 'top_k': 0, 'top_p': 1.0, 'seed': 7} -> if for NUM war those have also\n",
            "\n",
            "--- Samples BAC ---\n",
            "[BAC] w='i' cfg={'order': 3, 'temperature': 0.7, 'top_k': 60, 'top_p': 0.95, 'seed': 7} -> i am not the only one who is going to be a good time and i have been up to the school\n",
            "[BAC] w='my' cfg={'order': 3, 'temperature': 0.7, 'top_k': 60, 'top_p': 0.95, 'seed': 7} -> my friend\n",
            "[BAC] w='today' cfg={'order': 3, 'temperature': 0.7, 'top_k': 60, 'top_p': 0.95, 'seed': 7} -> today i went to the urllink blue pyramid\n",
            "[BAC] w='we' cfg={'order': 3, 'temperature': 0.7, 'top_k': 60, 'top_p': 0.95, 'seed': 7} -> we are going to be\n",
            "[BAC] w='i' cfg={'order': 3, 'temperature': 1.0, 'top_k': 60, 'top_p': 0.95, 'seed': 21} -> i am doing nothing but a really interesting stuff about me\n",
            "[BAC] w='my' cfg={'order': 3, 'temperature': 1.0, 'top_k': 60, 'top_p': 0.95, 'seed': 21} -> my dad who came out of here you are so many people who had been with the fact that theres someone involved in\n",
            "[BAC] w='today' cfg={'order': 3, 'temperature': 1.0, 'top_k': 60, 'top_p': 0.95, 'seed': 21} -> today i saw my face\n",
            "[BAC] w='we' cfg={'order': 3, 'temperature': 1.0, 'top_k': 60, 'top_p': 0.95, 'seed': 21} -> we had an affair\n",
            "[BAC] w='i' cfg={'order': 1, 'temperature': 1.0, 'top_k': 0, 'top_p': 1.0, 'seed': 7} -> i you the read people but\n",
            "[BAC] w='my' cfg={'order': 1, 'temperature': 1.0, 'top_k': 0, 'top_p': 1.0, 'seed': 7} -> my you the read people but\n",
            "[BAC] w='today' cfg={'order': 1, 'temperature': 1.0, 'top_k': 0, 'top_p': 1.0, 'seed': 7} -> today you the read people but\n",
            "[BAC] w='we' cfg={'order': 1, 'temperature': 1.0, 'top_k': 0, 'top_p': 1.0, 'seed': 7} -> we you the read people but\n",
            "\n",
            "[OK] Muestras guardadas en quinto-punto/sentence_samples.jsonl\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# Generador de oraciones (RAM<10GB) usando trigramas en SQLite + backoff\n",
        "# ===============================\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import gzip\n",
        "import math\n",
        "import sqlite3\n",
        "import random\n",
        "from functools import lru_cache\n",
        "from typing import Dict, List, Tuple, Optional, Callable\n",
        "\n",
        "cuarto_punto_folder: str = \"cuarto-punto\"\n",
        "group_code: str = \"group-ansada\"\n",
        "\n",
        "\n",
        "# ---------- Utils ----------\n",
        "def _json_load_any(path: str) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Carga un diccionario JSON {clave: prob} desde .json o .json.gz.\n",
        "\n",
        "    Args:\n",
        "        path: Ruta del archivo.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: Mapa de probabilidades.\n",
        "    \"\"\"\n",
        "    if path.endswith(\".gz\"):\n",
        "        with gzip.open(path, \"rt\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def _path(prefix: str, kind: str) -> str:\n",
        "    \"\"\"\n",
        "    Construye rutas a modelos.\n",
        "\n",
        "    Args:\n",
        "        prefix: \"20N\" o \"BAC\".\n",
        "        kind: uno de {\"uni\",\"bi\",\"tri_json\",\"tri_db\"}.\n",
        "\n",
        "    Returns:\n",
        "        str: Ruta al recurso solicitado.\n",
        "    \"\"\"\n",
        "    base = os.path.join(cuarto_punto_folder, \"models\", f\"{prefix}_{group_code}\")\n",
        "    if kind == \"uni\":\n",
        "        return base + \"_unigrams.json.gz\" if os.path.exists(base + \"_unigrams.json.gz\") else base + \"_unigrams.json\"\n",
        "    if kind == \"bi\":\n",
        "        return base + \"_bigrams.json.gz\" if os.path.exists(base + \"_bigrams.json.gz\") else base + \"_bigrams.json\"\n",
        "    if kind == \"tri_json\":\n",
        "        return base + \"_trigrams.json.gz\" if os.path.exists(base + \"_trigrams.json.gz\") else base + \"_trigrams.json\"\n",
        "    if kind == \"tri_db\":\n",
        "        return base + \"_trigrams.sqlite\"\n",
        "    raise ValueError(f\"kind inválido: {kind}\")\n",
        "\n",
        "\n",
        "# ---------- JSON(.gz) -> SQLite para trigramas (streaming) ----------\n",
        "def _trigram_json_to_sqlite(tri_json_path: str, tri_db_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Indexa trigramas {\"w1 w2 w3\": p} en SQLite (tabla tri(w1,w2,w3,p)) sin cargar todo en RAM.\n",
        "\n",
        "    Args:\n",
        "        tri_json_path: Ruta a *_trigrams.json(.gz).\n",
        "        tri_db_path: Ruta de salida *.sqlite.\n",
        "    \"\"\"\n",
        "    if os.path.exists(tri_db_path):\n",
        "        return\n",
        "    os.makedirs(os.path.dirname(tri_db_path), exist_ok=True)\n",
        "    conn = sqlite3.connect(tri_db_path)\n",
        "    cur = conn.cursor()\n",
        "    cur.executescript(\"\"\"\n",
        "        PRAGMA journal_mode=OFF;\n",
        "        PRAGMA synchronous=OFF;\n",
        "        PRAGMA temp_store=MEMORY;\n",
        "        CREATE TABLE tri (w1 TEXT, w2 TEXT, w3 TEXT, p REAL, PRIMARY KEY(w1,w2,w3));\n",
        "    \"\"\")\n",
        "\n",
        "    pat: re.Pattern[str] = re.compile(r'^\\s*\"([^\"]+)\":\\s*([0-9eE+\\-\\.]+)\\s*,?\\s*$')\n",
        "    batch: List[Tuple[str, str, str, float]] = []\n",
        "\n",
        "    fh = gzip.open(tri_json_path, \"rt\", encoding=\"utf-8\") if tri_json_path.endswith(\".gz\") \\\n",
        "         else open(tri_json_path, \"r\", encoding=\"utf-8\")\n",
        "    with fh:\n",
        "        _ = fh.readline()  # '{'\n",
        "        for line in fh:\n",
        "            s = line.strip()\n",
        "            if s == \"}\":\n",
        "                break\n",
        "            m = pat.match(s)\n",
        "            if not m:\n",
        "                continue\n",
        "            key, p_str = m.group(1), m.group(2)\n",
        "            parts = key.split(\" \")\n",
        "            if len(parts) != 3:\n",
        "                continue\n",
        "            batch.append((parts[0], parts[1], parts[2], float(p_str)))\n",
        "            if len(batch) >= 50_000:\n",
        "                cur.executemany(\"INSERT OR REPLACE INTO tri(w1,w2,w3,p) VALUES(?,?,?,?)\", batch)\n",
        "                conn.commit()\n",
        "                batch.clear()\n",
        "    if batch:\n",
        "        cur.executemany(\"INSERT OR REPLACE INTO tri(w1,w2,w3,p) VALUES(?,?,?,?)\", batch)\n",
        "        conn.commit()\n",
        "\n",
        "    cur.execute(\"ANALYZE\")\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(f\"[OK] Trigramas indexados: {tri_db_path}\")\n",
        "\n",
        "\n",
        "def _make_tri_access(prefix: str) -> Tuple[Callable[[str, str], List[Tuple[str, float]]], sqlite3.Connection]:\n",
        "    \"\"\"\n",
        "    Prepara acceso rápido a seguidores de trigramas vía SQLite.\n",
        "\n",
        "    Args:\n",
        "        prefix: \"20N\" o \"BAC\".\n",
        "\n",
        "    Returns:\n",
        "        (followers_fn, conn) donde followers_fn(w1,w2) -> [(w3, p), ...] ordenado por p desc.\n",
        "    \"\"\"\n",
        "    tri_json = _path(prefix, \"tri_json\")\n",
        "    tri_db = _path(prefix, \"tri_db\")\n",
        "    _trigram_json_to_sqlite(tri_json, tri_db)\n",
        "    conn = sqlite3.connect(tri_db)\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    @lru_cache(maxsize=200_000)\n",
        "    def followers(w1: str, w2: str) -> List[Tuple[str, float]]:\n",
        "        rows = cur.execute(\"SELECT w3,p FROM tri WHERE w1=? AND w2=?\", (w1, w2)).fetchall()\n",
        "        rows.sort(key=lambda x: x[1], reverse=True)\n",
        "        return [(str(w3), float(p)) for (w3, p) in rows]\n",
        "\n",
        "    return followers, conn\n",
        "\n",
        "\n",
        "# ---------- Muestreador ----------\n",
        "def _sample(\n",
        "    scored: List[Tuple[str, float]],\n",
        "    temperature: float = 0.9,\n",
        "    top_k: int = 50,\n",
        "    top_p: float = 0.95,\n",
        "    rng: Optional[random.Random] = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Muestrea un token desde una lista (token, score) con temperatura + top-k + nucleus (top-p).\n",
        "\n",
        "    Args:\n",
        "        scored: Candidatos (token, puntuación/probabilidad).\n",
        "        temperature: Suavizado (>=0). 1.0 = sin cambio.\n",
        "        top_k: recorta a los k mejores (si >0).\n",
        "        top_p: recorta por prob acumulada (0<p<=1).\n",
        "        rng: generador aleatorio (reproducible si se pasa semilla).\n",
        "\n",
        "    Returns:\n",
        "        str: token elegido (o </s> si no hay candidatos).\n",
        "    \"\"\"\n",
        "    if not scored:\n",
        "        return \"</s>\"\n",
        "    if rng is None:\n",
        "        rng = random\n",
        "\n",
        "    # Orden y top-k\n",
        "    scored = sorted(scored, key=lambda x: x[1], reverse=True)\n",
        "    if top_k and top_k > 0:\n",
        "        scored = scored[:min(top_k, len(scored))]\n",
        "\n",
        "    # Normaliza a prob y aplica top-p (nucleus)\n",
        "    total = sum(max(0.0, p) for _, p in scored) or 1e-12\n",
        "    probs = [max(0.0, p) / total for _, p in scored]\n",
        "\n",
        "    cut: List[Tuple[str, float]] = []\n",
        "    acc = 0.0\n",
        "    for (tok, _), p in zip(scored, probs):\n",
        "        cut.append((tok, p))\n",
        "        acc += p\n",
        "        if top_p is not None and acc >= top_p:\n",
        "            break\n",
        "\n",
        "    # Temperatura sobre log-prob\n",
        "    logs = [math.log(max(1e-12, p)) / max(1e-6, temperature) for _, p in cut]\n",
        "    m = max(logs)\n",
        "    exps = [math.exp(x - m) for x in logs]\n",
        "    z = sum(exps)\n",
        "    adj = [e / z for e in exps]\n",
        "\n",
        "    r = rng.random()\n",
        "    c = 0.0\n",
        "    for (tok, _), p in zip(cut, adj):\n",
        "        c += p\n",
        "        if r <= c:\n",
        "            return tok\n",
        "    return cut[-1][0]\n",
        "\n",
        "\n",
        "# ---------- Generador (elige orden 1/2/3; por defecto 3 con backoff) ----------\n",
        "def build_sentence_generator(prefix: str = \"20N\", default_seed: Optional[int] = 7):\n",
        "    \"\"\"\n",
        "    Construye un generador de oraciones para el corpus dado.\n",
        "\n",
        "    Carga unigrama y bigrama en RAM; trigramas se consultan en SQLite.\n",
        "    Por defecto genera con TRIGRAMAS (mejor fluidez) y backoff a uni.\n",
        "\n",
        "    Args:\n",
        "        prefix: \"20N\" o \"BAC\".\n",
        "        default_seed: Semilla por defecto para reproducibilidad.\n",
        "\n",
        "    Returns:\n",
        "        (gen_fn, close_fn)\n",
        "          - gen_fn(first_word, max_len=30, temperature=0.9, top_k=60, top_p=0.95,\n",
        "                  seed=None, order=3) -> (sentence, tokens)\n",
        "          - close_fn(): cierra la conexión SQLite interna.\n",
        "    \"\"\"\n",
        "    uni: Dict[str, float] = _json_load_any(_path(prefix, \"uni\"))      # {\"w\": p}\n",
        "    bi: Dict[str, float] = _json_load_any(_path(prefix, \"bi\"))        # {\"w1 w2\": p}\n",
        "    tri_followers, tri_conn = _make_tri_access(prefix)\n",
        "\n",
        "    uni_items: List[Tuple[str, float]] = [(w, float(p)) for w, p in uni.items()]\n",
        "\n",
        "    def gen(\n",
        "        first_word: str,\n",
        "        max_len: int = 30,\n",
        "        temperature: float = 0.9,\n",
        "        top_k: int = 60,\n",
        "        top_p: float = 0.95,\n",
        "        seed: Optional[int] = default_seed,\n",
        "        order: int = 3\n",
        "    ) -> Tuple[str, List[str]]:\n",
        "        \"\"\"\n",
        "        Genera una oración dada la primera palabra.\n",
        "\n",
        "        Args:\n",
        "            first_word: Primera palabra de la oración (se normaliza a minúsculas).\n",
        "            max_len: longitud máxima de tokens (incluye etiquetas).\n",
        "            temperature: parámetro de muestreo.\n",
        "            top_k: recorte por k mejores candidatos.\n",
        "            top_p: nucleus sampling.\n",
        "            seed: semilla para reproducibilidad.\n",
        "            order: 1=unigrama (best-by-perplexity), 2=bigrama, 3=trigrama con backoff.\n",
        "\n",
        "        Returns:\n",
        "            (superficie_sin_etiquetas, lista_de_tokens_incluyendo_<s>/</s>)\n",
        "        \"\"\"\n",
        "        rng = random.Random(seed)\n",
        "        fw = first_word.strip().lower()\n",
        "        if fw not in uni:\n",
        "            fw = \"<UNK>\" if \"<UNK>\" in uni else fw\n",
        "\n",
        "        tokens: List[str] = [\"<s>\", fw]\n",
        "\n",
        "        def _next_from_bigram(a: str) -> str:\n",
        "            # candidatos del bigrama a->?\n",
        "            cands = [(w2, float(p)) for (w12, p) in bi.items() if w12.startswith(a + \" \")]\n",
        "            return _sample(cands if cands else uni_items, temperature, top_k, top_p, rng)\n",
        "\n",
        "        while len(tokens) < max_len:\n",
        "            if tokens[-1] == \"</s>\":\n",
        "                break\n",
        "\n",
        "            if order == 1:\n",
        "                nxt = _sample(uni_items, temperature, top_k, top_p, rng)\n",
        "\n",
        "            elif order == 2:\n",
        "                prev1 = tokens[-1]\n",
        "                nxt = _next_from_bigram(prev1)\n",
        "\n",
        "            else:  # order == 3 (por defecto)\n",
        "                prev2 = tokens[-2] if len(tokens) >= 2 else \"<s>\"\n",
        "                prev1 = tokens[-1]\n",
        "                cand = tri_followers(prev2, prev1)\n",
        "                if cand:\n",
        "                    nxt = _sample(cand, temperature, top_k, top_p, rng)\n",
        "                else:\n",
        "                    # backoff simple a unigramas si no hay contexto\n",
        "                    nxt = _sample(uni_items, temperature, top_k, top_p, rng)\n",
        "\n",
        "            tokens.append(nxt)\n",
        "            if len(tokens) >= max_len - 1 and \"</s>\" in uni:\n",
        "                tokens.append(\"</s>\")\n",
        "\n",
        "        surface = [t for t in tokens if t not in (\"<s>\", \"</s>\")]\n",
        "        return \" \".join(surface), tokens\n",
        "\n",
        "    def close() -> None:\n",
        "        \"\"\"Cierra recursos asociados (conexión SQLite).\"\"\"\n",
        "        try:\n",
        "            tri_conn.close()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    return gen, close\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# PRUEBAS (documentación breve)\n",
        "# ===============================\n",
        "if __name__ == '__main__':\n",
        "    os.makedirs(\"quinto-punto\", exist_ok=True)\n",
        "\n",
        "    tests = [\n",
        "        (\"20N\", [\"the\", \"this\", \"i\", \"if\"]),\n",
        "        (\"BAC\", [\"i\", \"my\", \"today\", \"we\"]),\n",
        "    ]\n",
        "    configs = [\n",
        "        # Calidad (trigrama backoff)\n",
        "        {\"order\": 3, \"temperature\": 0.7, \"top_k\": 60, \"top_p\": 0.95, \"seed\": 7},\n",
        "        {\"order\": 3, \"temperature\": 1.0, \"top_k\": 60, \"top_p\": 0.95, \"seed\": 21},\n",
        "        # Estricto “best-by-perplexity” (unigrama)\n",
        "        {\"order\": 1, \"temperature\": 1.0, \"top_k\": 0, \"top_p\": 1.0, \"seed\": 7},\n",
        "    ]\n",
        "\n",
        "    report_lines: List[Dict[str, object]] = []\n",
        "    for prefix, starters in tests:\n",
        "        gen, close = build_sentence_generator(prefix)\n",
        "        print(f\"\\n--- Samples {prefix} ---\")\n",
        "        for cfg in configs:\n",
        "            for w in starters:\n",
        "                s, toks = gen(w, max_len=25, **cfg)  # type: ignore[arg-type]\n",
        "                print(f\"[{prefix}] w='{w}' cfg={cfg} -> {s}\")\n",
        "                report_lines.append({\n",
        "                    \"corpus\": prefix, \"first_word\": w, **cfg,\n",
        "                    \"max_len\": 25, \"sentence\": s, \"tokens\": toks\n",
        "                })\n",
        "        close()\n",
        "\n",
        "    # Guarda las muestras (documentación de pruebas)\n",
        "    with open(\"quinto-punto/sentence_samples.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in report_lines:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "    print(\"\\n[OK] Muestras guardadas en quinto-punto/sentence_samples.jsonl\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

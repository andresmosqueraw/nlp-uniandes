{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b982de2",
   "metadata": {},
   "source": [
    "(15p) Using the test dataset, calculate the perplexity of each language model. Report the results obtained. If you experience variable overflow, use probabilities in log space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e66832",
   "metadata": {},
   "source": [
    "El link de la carpeta donde estan los conjuntos de entrenamiento + testing y los modelos entrenados es:\n",
    "\n",
    "https://uniandes-my.sharepoint.com/:f:/g/personal/a_mosquerah2_uniandes_edu_co/Em-od1gldI9BnnTjpUXqZXcB8fjXUoybI35zktWPWzyqpw?e=6dT4ng\n",
    "\n",
    "(Solo se puede abrir con correo uniandes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e990da19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carpeta conjunto de entrenamiento y de testing de 20news y de BAC\n",
    "tercer_punto_folder = \"tercer-punto\"\n",
    "# carpeta modelos de unigramas, bigramas y trigramas de cada corpus. \n",
    "cuarto_punto_folder = \"cuarto-punto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a544a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Calculando la perplejidad ---\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/20N_my_group_unigrams.json'\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/20N_my_group_bigrams.json'\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/20N_my_group_trigrams.json'\n",
      "\n",
      "Resultados de Perplejidad para 20N (Corpus):\n",
      "  Unigramas: 1095.89\n",
      "  Bigramas:  2327.66\n",
      "  Trigramas: 11091.42\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/BAC_my_group_unigrams.json'\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/BAC_my_group_bigrams.json'\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/BAC_my_group_trigrams.json'\n",
      "\n",
      "Resultados de Perplejidad para BAC (Corpus):\n",
      "  Unigramas: 850.07\n",
      "  Bigramas:  1215.92\n",
      "  Trigramas: 13157.86\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Tuple, Optional, Any\n",
    "\n",
    "\n",
    "def read_ngram_model(file_path: str) -> Optional[Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Carga un modelo de n-gramas desde un archivo JSON.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Ruta del archivo JSON que contiene el modelo.\n",
    "\n",
    "    Returns:\n",
    "        Optional[Dict[str, float]]: Diccionario con las probabilidades del modelo\n",
    "        o None si ocurre un error al leer el archivo.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            model: Dict[str, float] = json.load(f)\n",
    "        print(f\"[OK] Modelo cargado desde '{file_path}'\")\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] Archivo no encontrado: '{file_path}'\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] No se pudo cargar el modelo desde '{file_path}': {e}\")\n",
    "        return None\n",
    "\n",
    "def _normalize_ngram_keys(\n",
    "    model_dict: Dict[str, float], n: int\n",
    ") -> Tuple[Dict[Tuple[str, ...], float], Dict[Any, int]]:\n",
    "    \"\"\"\n",
    "    Convierte llaves de un diccionario de n-gramas en tuplas para acceso O(1)\n",
    "    y calcula el número de tipos de tokens siguientes por contexto.\n",
    "\n",
    "    Args:\n",
    "        model_dict (Dict[str, float]): Diccionario del modelo con claves en formato string.\n",
    "        n (int): Tamaño del n-grama (1, 2 o 3).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[Tuple[str, ...], float], Dict[Any, int]]:\n",
    "            - Diccionario con claves como tuplas y sus probabilidades.\n",
    "            - Diccionario con el número de tokens siguientes por contexto.\n",
    "    \"\"\"\n",
    "    out: Dict[Tuple[str, ...], float] = {}\n",
    "    followers: Dict[Any, int] = defaultdict(int)\n",
    "\n",
    "    if n == 1:\n",
    "        for k, v in model_dict.items():\n",
    "            out[(k,)] = float(v)\n",
    "        return out, {}\n",
    "\n",
    "    elif n == 2:\n",
    "        next_types = defaultdict(set)\n",
    "        for k, v in model_dict.items():\n",
    "            a, b = k.split(\" \", 1)\n",
    "            out[(a, b)] = float(v)\n",
    "            next_types[a].add(b)\n",
    "        followers = {a: len(bs) for a, bs in next_types.items()}\n",
    "        return out, followers\n",
    "\n",
    "    elif n == 3:\n",
    "        next_types = defaultdict(set)\n",
    "        for k, v in model_dict.items():\n",
    "            a, b, c = k.split(\" \", 2)\n",
    "            out[(a, b, c)] = float(v)\n",
    "            next_types[(a, b)].add(c)\n",
    "        followers = {ctx: len(cs) for ctx, cs in next_types.items()}\n",
    "        return out, followers\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"n debe ser 1, 2 o 3\")\n",
    "    \n",
    "\n",
    "def calculate_perplexity(\n",
    "    testing_file: str,\n",
    "    unigram_model: Dict[str, float],\n",
    "    bigram_model: Dict[str, float],\n",
    "    trigram_model: Dict[str, float],\n",
    "    vocab_size: Optional[int] = None,\n",
    ") -> Tuple[Optional[float], Optional[float], Optional[float]]:\n",
    "    \"\"\"\n",
    "    Calcula la perplejidad de un corpus de prueba usando modelos de unigramas,\n",
    "    bigramas y trigramas con suavizado de Laplace.\n",
    "\n",
    "    Args:\n",
    "        testing_file (str): Ruta del archivo con el corpus de prueba.\n",
    "        unigram_model (Dict[str, float]): Modelo de unigramas.\n",
    "        bigram_model (Dict[str, float]): Modelo de bigramas.\n",
    "        trigram_model (Dict[str, float]): Modelo de trigramas.\n",
    "        vocab_size (Optional[int], opcional): Tamaño del vocabulario. \n",
    "                                              Si es None, se calcula a partir de los unigramas.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Optional[float], Optional[float], Optional[float]]:\n",
    "            Perplejidad de unigramas, bigramas y trigramas.\n",
    "            Devuelve None si ocurre un error, o inf si el corpus está vacío.\n",
    "    \"\"\"\n",
    "    if any(m is None for m in (unigram_model, bigram_model, trigram_model)):\n",
    "        return None, None, None\n",
    "\n",
    "    uni, _ = _normalize_ngram_keys(unigram_model, 1)\n",
    "    bi, followers_bi = _normalize_ngram_keys(bigram_model, 2)\n",
    "    tri, followers_tri = _normalize_ngram_keys(trigram_model, 3)\n",
    "\n",
    "    if vocab_size is None:\n",
    "        vocab_size = len(uni)\n",
    "\n",
    "    laplace_uni_denom: int = len(uni) + vocab_size\n",
    "\n",
    "    total_log_u: float = 0.0\n",
    "    total_log_b: float = 0.0\n",
    "    total_log_t: float = 0.0\n",
    "    total_tokens: int = 0\n",
    "\n",
    "    try:\n",
    "        with open(testing_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for raw in f:\n",
    "                s: str = raw.strip()\n",
    "                if not s:\n",
    "                    continue\n",
    "                tokens = s.split()\n",
    "                n: int = len(tokens)\n",
    "                if n == 0:\n",
    "                    continue\n",
    "\n",
    "                total_tokens += n\n",
    "\n",
    "                # -------- Unigram --------\n",
    "                for w in tokens:\n",
    "                    p = uni.get((w,), 0.0)\n",
    "                    if p <= 0.0:\n",
    "                        p = 1.0 / laplace_uni_denom\n",
    "                    total_log_u += math.log(p)\n",
    "\n",
    "                # -------- Bigram --------\n",
    "                # P(w1) + Π P(w_i | w_{i-1})\n",
    "                # w1 como unigrama\n",
    "                p1 = uni.get((tokens[0],), 0.0)\n",
    "                if p1 <= 0.0:\n",
    "                    p1 = 1.0 / laplace_uni_denom\n",
    "                total_log_b += math.log(p1)\n",
    "\n",
    "                for i in range(n - 1):\n",
    "                    a, b_ = tokens[i], tokens[i + 1]\n",
    "                    p = bi.get((a, b_), 0.0)\n",
    "                    if p <= 0.0:\n",
    "                        context_types = followers_bi.get(a, 0)\n",
    "                        p = 1.0 / (context_types + vocab_size)\n",
    "                    total_log_b += math.log(p)\n",
    "\n",
    "                # -------- Trigram --------\n",
    "                # P(w1) * P(w2|w1) * Π P(w_i | w_{i-2}, w_{i-1})\n",
    "                # w1 como unigrama\n",
    "                p1 = uni.get((tokens[0],), 0.0)\n",
    "                if p1 <= 0.0:\n",
    "                    p1 = 1.0 / laplace_uni_denom\n",
    "                total_log_t += math.log(p1)\n",
    "\n",
    "                if n >= 2:\n",
    "                    p2 = bi.get((tokens[0], tokens[1]), 0.0)\n",
    "                    if p2 <= 0.0:\n",
    "                        ctx_types = followers_bi.get(tokens[0], 0)\n",
    "                        p2 = 1.0 / (ctx_types + vocab_size)\n",
    "                    total_log_t += math.log(p2)\n",
    "\n",
    "                for i in range(n - 2):\n",
    "                    a, b_, c = tokens[i], tokens[i + 1], tokens[i + 2]\n",
    "                    p = tri.get((a, b_, c), 0.0)\n",
    "                    if p <= 0.0:\n",
    "                        ctx_types = followers_tri.get((a, b_), 0)\n",
    "                        p = 1.0 / (ctx_types + vocab_size)\n",
    "                    total_log_t += math.log(p)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] No se pudo leer el archivo de prueba '{testing_file}': {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    if total_tokens == 0:\n",
    "        return float(\"inf\"), float(\"inf\"), float(\"inf\")\n",
    "\n",
    "    ppl_u: float = math.exp(-total_log_u / total_tokens)\n",
    "    ppl_b: float = math.exp(-total_log_b / total_tokens)\n",
    "    ppl_t: float = math.exp(-total_log_t / total_tokens)\n",
    "    return ppl_u, ppl_b, ppl_t\n",
    "\n",
    "# ---------- Script principal ----------\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    group_code = \"my_group\"\n",
    "\n",
    "    training_file_20N = os.path.join(tercer_punto_folder, f\"20N_{group_code}_training.txt\")\n",
    "    testing_file_20N  = os.path.join(tercer_punto_folder, f\"20N_{group_code}_testing.txt\")\n",
    "    training_file_BAC = os.path.join(tercer_punto_folder, f\"BAC_{group_code}_training.txt\")\n",
    "    testing_file_BAC  = os.path.join(tercer_punto_folder, f\"BAC_{group_code}_testing.txt\")\n",
    "\n",
    "    print(\"\\n\\n--- Calculando la perplejidad ---\")\n",
    "\n",
    "    # Modelos 20N\n",
    "    unigrams_20N = read_ngram_model(os.path.join(cuarto_punto_folder, \"models\", f\"20N_{group_code}_unigrams.json\"))\n",
    "    bigrams_20N  = read_ngram_model(os.path.join(cuarto_punto_folder, \"models\", f\"20N_{group_code}_bigrams.json\"))\n",
    "    trigrams_20N = read_ngram_model(os.path.join(cuarto_punto_folder, \"models\", f\"20N_{group_code}_trigrams.json\"))\n",
    "\n",
    "    # vocab: usa #unigramas; evita re-leer el corpus completo\n",
    "    vocab_size_20N = len(unigrams_20N) if unigrams_20N else None\n",
    "\n",
    "    if all([unigrams_20N, bigrams_20N, trigrams_20N]):\n",
    "        pp_uni_20N, pp_bi_20N, pp_tri_20N = calculate_perplexity(\n",
    "            testing_file_20N,\n",
    "            unigrams_20N, bigrams_20N, trigrams_20N,\n",
    "            vocab_size=vocab_size_20N\n",
    "        )\n",
    "        print(f\"\\nResultados de Perplejidad para 20N (Corpus):\")\n",
    "        print(f\"  Unigramas: {pp_uni_20N:.2f}\")\n",
    "        print(f\"  Bigramas:  {pp_bi_20N:.2f}\")\n",
    "        print(f\"  Trigramas: {pp_tri_20N:.2f}\")\n",
    "\n",
    "    # Modelos BAC\n",
    "    unigrams_BAC = read_ngram_model(os.path.join(cuarto_punto_folder, \"models\", f\"BAC_{group_code}_unigrams.json\"))\n",
    "    bigrams_BAC  = read_ngram_model(os.path.join(cuarto_punto_folder, \"models\", f\"BAC_{group_code}_bigrams.json\"))\n",
    "    trigrams_BAC = read_ngram_model(os.path.join(cuarto_punto_folder, \"models\", f\"BAC_{group_code}_trigrams.json\"))\n",
    "\n",
    "    vocab_size_BAC = len(unigrams_BAC) if unigrams_BAC else None\n",
    "\n",
    "    if all([unigrams_BAC, bigrams_BAC, trigrams_BAC]):\n",
    "        pp_uni_BAC, pp_bi_BAC, pp_tri_BAC = calculate_perplexity(\n",
    "            testing_file_BAC,\n",
    "            unigrams_BAC, bigrams_BAC, trigrams_BAC,\n",
    "            vocab_size=vocab_size_BAC\n",
    "        )\n",
    "        print(f\"\\nResultados de Perplejidad para BAC (Corpus):\")\n",
    "        print(f\"  Unigramas: {pp_uni_BAC:.2f}\")\n",
    "        print(f\"  Bigramas:  {pp_bi_BAC:.2f}\")\n",
    "        print(f\"  Trigramas: {pp_tri_BAC:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebe989d",
   "metadata": {},
   "source": [
    "(15p) Using your best language model, build a method/function that automatically generates sentences by receiving the first word of a sentence as input. Take different tests and document them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a902576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20N|tri-only] 'the': the fact that he was still too cold\n",
      "[20N|tri-only] 'this': this is a different kind of scary when you have a NUM bit serial converters someone was kind to ones intuition can be\n",
      "[20N|tri-only] 'i': i am not able to run\n",
      "[20N|tri-only] 'if': if you have an NUM simulator on an absolute truth is that the majority rather than a pawn of the new reform party\n",
      "[BAC|tri-only] 'i': i was a big deal to you by urllink quizilla urllink what kind of strange\n",
      "[BAC|tri-only] 'my': my first day i was able to get some sleep\n",
      "[BAC|tri-only] 'today': today i was supposed to be in the back of my life\n",
      "[BAC|tri-only] 'we': we have to admit that im going to get a good thing\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Set, Tuple, List, Optional, Callable, Any\n",
    "\n",
    "\n",
    "def _load_trigram_unigram(\n",
    "    prefix: str = \"20N\", \n",
    "    group_code: str = \"my_group\", \n",
    "    base_dir: str = \"cuarto-punto/models\"\n",
    ") -> Tuple[Dict[Tuple[str, str, str], float], Dict[Tuple[str, str], Set[str]], Dict[Tuple[str], float], Set[str]]:\n",
    "    \"\"\"\n",
    "    Carga modelos de trigramas y unigramas desde archivos JSON.\n",
    "\n",
    "    Args:\n",
    "        prefix (str, opcional): Prefijo del corpus (ejemplo: \"20N\" o \"BAC\").\n",
    "        group_code (str, opcional): Código de grupo para diferenciar los modelos.\n",
    "        base_dir (str, opcional): Directorio base donde se almacenan los modelos.\n",
    "\n",
    "    Returns:\n",
    "        Tuple:\n",
    "            - Dict[Tuple[str, str, str], float]: Diccionario de trigramas con probabilidades.\n",
    "            - Dict[Tuple[str, str], Set[str]]: Diccionario de contextos de trigramas con tokens siguientes.\n",
    "            - Dict[Tuple[str], float]: Diccionario de unigramas con probabilidades.\n",
    "            - Set[str]: Conjunto del vocabulario.\n",
    "    \"\"\"\n",
    "    with open(os.path.join(base_dir, f\"{prefix}_{group_code}_trigrams.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        tri_raw: Dict[str, float] = json.load(f)\n",
    "    with open(os.path.join(base_dir, f\"{prefix}_{group_code}_unigrams.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        uni_raw: Dict[str, float] = json.load(f)\n",
    "\n",
    "    tri: Dict[Tuple[str, str, str], float] = {}\n",
    "    followers: Dict[Tuple[str, str], Set[str]] = defaultdict(set)\n",
    "    for k, v in tri_raw.items():\n",
    "        a, b, c = k.split(\" \", 2)\n",
    "        tri[(a, b, c)] = float(v)\n",
    "        followers[(a, b)].add(c)\n",
    "\n",
    "    uni: Dict[Tuple[str], float] = {(k,): float(v) for k, v in uni_raw.items()}\n",
    "    vocab: Set[str] = {k[0] for k in uni.keys()}\n",
    "    return tri, followers, uni, vocab\n",
    "\n",
    "\n",
    "def _sample(\n",
    "    scored: List[Tuple[str, float]], \n",
    "    temperature: float = 0.9, \n",
    "    top_k: int = 50, \n",
    "    top_p: float = 0.95, \n",
    "    rng: Optional[random.Random] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Realiza un muestreo de tokens usando temperatura, top-k y top-p.\n",
    "\n",
    "    Args:\n",
    "        scored (List[Tuple[str, float]]): Lista de pares (token, score).\n",
    "        temperature (float, opcional): Factor de suavizado de probabilidades.\n",
    "        top_k (int, opcional): Número máximo de tokens candidatos.\n",
    "        top_p (float, opcional): Probabilidad acumulada mínima para aplicar nucleus sampling.\n",
    "        rng (Optional[random.Random], opcional): Generador de números aleatorios reproducible.\n",
    "\n",
    "    Returns:\n",
    "        str: Token seleccionado.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = random\n",
    "\n",
    "    scored = sorted(scored, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if top_k and top_k > 0:\n",
    "        scored = scored[:min(top_k, len(scored))]\n",
    "\n",
    "    total: float = sum(s for _, s in scored) or 1e-12\n",
    "    probs: List[float] = [s / total for _, s in scored]\n",
    "\n",
    "    cut: List[Tuple[str, float]] = []\n",
    "    acc: float = 0.0\n",
    "    for (t, s), p in zip(scored, probs):\n",
    "        cut.append((t, s))\n",
    "        acc += p\n",
    "        if top_p is not None and acc >= top_p:\n",
    "            break\n",
    "    scored = cut\n",
    "\n",
    "    logs: List[float] = [math.log(max(1e-12, s)) / max(1e-6, temperature) for _, s in scored]\n",
    "    m: float = max(logs)\n",
    "    exps: List[float] = [math.exp(x - m) for x in logs]\n",
    "    z: float = sum(exps)\n",
    "    probs = [e / z for e in exps]\n",
    "\n",
    "    r: float = rng.random()\n",
    "    acc = 0.0\n",
    "    for (tok, _), p in zip(scored, probs):\n",
    "        acc += p\n",
    "        if r <= acc:\n",
    "            return tok\n",
    "    return scored[-1][0]\n",
    "\n",
    "\n",
    "def build_trigram_only_generator(\n",
    "    prefix: str = \"20N\", \n",
    "    group_code: str = \"my_group\", \n",
    "    base_dir: str = \"cuarto-punto/models\"\n",
    ") -> Callable[..., Tuple[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Construye un generador de oraciones basado únicamente en trigramas\n",
    "    con retroceso a unigramas en caso de que el contexto no exista.\n",
    "\n",
    "    Args:\n",
    "        prefix (str, opcional): Prefijo del corpus (ejemplo: \"20N\" o \"BAC\").\n",
    "        group_code (str, opcional): Código de grupo para diferenciar modelos.\n",
    "        base_dir (str, opcional): Directorio base donde se almacenan los modelos.\n",
    "\n",
    "    Returns:\n",
    "        Callable[..., Tuple[str, List[str]]]: Función generadora que recibe\n",
    "        la primera palabra y devuelve la oración generada y la lista de tokens.\n",
    "    \"\"\"\n",
    "    tri, followers, uni, vocab = _load_trigram_unigram(prefix, group_code, base_dir)\n",
    "\n",
    "    def gen(\n",
    "        first_word: str, \n",
    "        max_len: int = 30, \n",
    "        temperature: float = 0.9, \n",
    "        top_k: int = 50, \n",
    "        top_p: float = 0.95, \n",
    "        seed: Optional[int] = None\n",
    "    ) -> Tuple[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Genera una oración a partir de una palabra inicial.\n",
    "\n",
    "        Args:\n",
    "            first_word (str): Primera palabra de la oración.\n",
    "            max_len (int, opcional): Longitud máxima de la oración.\n",
    "            temperature (float, opcional): Factor de suavizado de probabilidades.\n",
    "            top_k (int, opcional): Número máximo de candidatos para muestreo top-k.\n",
    "            top_p (float, opcional): Probabilidad acumulada mínima para nucleus sampling.\n",
    "            seed (Optional[int], opcional): Semilla para reproducibilidad.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[str, List[str]]:\n",
    "                - str: Oración generada sin etiquetas.\n",
    "                - List[str]: Tokens generados incluyendo etiquetas <s> y </s>.\n",
    "        \"\"\"\n",
    "        rng = random.Random(seed)\n",
    "        fw: str = first_word.strip().lower()\n",
    "        if fw not in vocab:\n",
    "            fw = \"<unk>\" if \"<unk>\" in vocab else fw\n",
    "\n",
    "        tokens: List[str] = [\"<s>\", fw]\n",
    "        while len(tokens) < max_len:\n",
    "            if tokens[-1] == \"</s>\":\n",
    "                break\n",
    "            prev2: str = tokens[-2] if len(tokens) >= 2 else \"<s>\"\n",
    "            prev1: str = tokens[-1]\n",
    "\n",
    "            cand: Optional[Set[str]] = followers.get((prev2, prev1), None)\n",
    "            if cand:\n",
    "                scored: List[Tuple[str, float]] = [(w, tri.get((prev2, prev1, w), 0.0)) for w in cand]\n",
    "                nxt: str = _sample(scored, temperature, top_k, top_p, rng)\n",
    "            else:\n",
    "                scored = [(w, uni.get((w,), 0.0)) for w in vocab]\n",
    "                nxt = _sample(scored, temperature, top_k, top_p, rng)\n",
    "\n",
    "            tokens.append(nxt)\n",
    "            if len(tokens) >= max_len - 1 and \"</s>\" in vocab:\n",
    "                tokens.append(\"</s>\")\n",
    "                break\n",
    "\n",
    "        surface: List[str] = [t for t in tokens if t not in (\"<s>\", \"</s>\")]\n",
    "        return \" \".join(surface), tokens\n",
    "\n",
    "    return gen\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gen_tri_20n = build_trigram_only_generator(prefix=\"20N\", group_code=\"my_group\")\n",
    "    gen_tri_bac = build_trigram_only_generator(prefix=\"BAC\", group_code=\"my_group\")\n",
    "\n",
    "    for w in [\"the\", \"this\", \"i\", \"if\"]:\n",
    "        s, _ = gen_tri_20n(w, max_len=25, temperature=0.9, top_k=60, top_p=0.95, seed=7)\n",
    "        print(f\"[20N|tri-only] '{w}': {s}\")\n",
    "\n",
    "    for w in [\"i\", \"my\", \"today\", \"we\"]:\n",
    "        s, _ = gen_tri_bac(w, max_len=25, temperature=0.9, top_k=60, top_p=0.95, seed=7)\n",
    "        print(f\"[BAC|tri-only] '{w}': {s}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

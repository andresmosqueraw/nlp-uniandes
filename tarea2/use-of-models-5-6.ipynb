{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b982de2",
   "metadata": {},
   "source": [
    "(15p) Using the test dataset, calculate the perplexity of each language model. Report the results obtained. If you experience variable overflow, use probabilities in log space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "e4a544a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Calculando la perplejidad ---\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/20N_my_group_unigrams.json'\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/20N_my_group_bigrams.json'\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/20N_my_group_trigrams.json'\n",
      "\n",
      "Resultados de Perplejidad para 20N (Corpus):\n",
      "  Unigramas: 1256.35\n",
      "  Bigramas:  3104.00\n",
      "  Trigramas: 15012.65\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/BAC_my_group_unigrams.json'\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/BAC_my_group_bigrams.json'\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/BAC_my_group_trigrams.json'\n",
      "\n",
      "Resultados de Perplejidad para BAC (Corpus):\n",
      "  Unigramas: 825.99\n",
      "  Bigramas:  1163.46\n",
      "  Trigramas: 12544.12\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------- Utilidades de carga/normalización ----------\n",
    "\n",
    "def read_ngram_model(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            model = json.load(f)\n",
    "        print(f\"[OK] Modelo cargado desde '{file_path}'\")\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] Archivo no encontrado: '{file_path}'\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] No se pudo cargar el modelo desde '{file_path}': {e}\")\n",
    "        return None\n",
    "\n",
    "def _normalize_ngram_keys(model_dict, n):\n",
    "    \"\"\"\n",
    "    Convierte llaves 'a b' / 'a b c' en tuplas ('a','b'[, 'c']) para O(1) consistente y evita splits repetidos.\n",
    "    Devuelve (dict_normalizado, seguidores_por_contexto)\n",
    "    seguidores_por_contexto:\n",
    "        - bigrama: dict prev -> #tipos_siguientes\n",
    "        - trigrama: dict (prev1,prev2) -> #tipos_siguientes\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    followers = defaultdict(int)\n",
    "    if n == 1:\n",
    "        # unigramas ya están por token -> p(token)\n",
    "        # no se necesita followers\n",
    "        for k, v in model_dict.items():\n",
    "            out[(k,)] = float(v)\n",
    "        return out, {}\n",
    "    elif n == 2:\n",
    "        next_types = defaultdict(set)\n",
    "        for k, v in model_dict.items():\n",
    "            a, b = k.split(' ', 1)\n",
    "            out[(a, b)] = float(v)\n",
    "            next_types[a].add(b)\n",
    "        followers = {a: len(bs) for a, bs in next_types.items()}\n",
    "        return out, followers\n",
    "    elif n == 3:\n",
    "        next_types = defaultdict(set)\n",
    "        for k, v in model_dict.items():\n",
    "            a, b, c = k.split(' ', 2)\n",
    "            out[(a, b, c)] = float(v)\n",
    "            next_types[(a, b)].add(c)\n",
    "        followers = {ctx: len(cs) for ctx, cs in next_types.items()}\n",
    "        return out, followers\n",
    "    else:\n",
    "        raise ValueError(\"n debe ser 1, 2 o 3\")\n",
    "\n",
    "# ---------- Perplejidad optimizada ----------\n",
    "\n",
    "def calculate_perplexity(\n",
    "    testing_file,\n",
    "    unigram_model, bigram_model, trigram_model,\n",
    "    vocab_size=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Calcula perplejidad usando:\n",
    "      - Accesos O(1) (tuplas como claves)\n",
    "      - Conteos de contexto precomputados (evita sum(...) por llave)\n",
    "      - Log-probabilidades para estabilidad\n",
    "      - Lectura streaming del archivo de prueba\n",
    "    \"\"\"\n",
    "    if any(m is None for m in (unigram_model, bigram_model, trigram_model)):\n",
    "        return None, None, None\n",
    "\n",
    "    # Normaliza llaves y precomputa seguidores/contexts\n",
    "    uni, _ = _normalize_ngram_keys(unigram_model, 1)\n",
    "    bi, followers_bi = _normalize_ngram_keys(bigram_model, 2)\n",
    "    tri, followers_tri = _normalize_ngram_keys(trigram_model, 3)\n",
    "\n",
    "    # Tamaño de vocabulario: por defecto = #unigramas distintos\n",
    "    if vocab_size is None:\n",
    "        vocab_size = len(uni)\n",
    "\n",
    "    # Precalcula denominadores de Laplace para velocidad\n",
    "    # Nota: tu fórmula original usa (#tipos_siguientes + V) como denominador de suavizado.\n",
    "    # Mantengo esa lógica para equivalencia funcional, pero ahora O(1).\n",
    "    laplace_uni_denom = (len(uni) + vocab_size)\n",
    "\n",
    "    total_log_u = 0.0\n",
    "    total_log_b = 0.0\n",
    "    total_log_t = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Lectura streaming (sin cargar todo a memoria)\n",
    "    try:\n",
    "        with open(testing_file, 'r', encoding='utf-8') as f:\n",
    "            for raw in f:\n",
    "                s = raw.strip()\n",
    "                if not s:\n",
    "                    continue\n",
    "                tokens = s.split()\n",
    "                n = len(tokens)\n",
    "                if n == 0:\n",
    "                    continue\n",
    "\n",
    "                total_tokens += n\n",
    "\n",
    "                # -------- Unigram --------\n",
    "                for w in tokens:\n",
    "                    p = uni.get((w,), 0.0)\n",
    "                    if p <= 0.0:\n",
    "                        p = 1.0 / laplace_uni_denom\n",
    "                    total_log_u += math.log(p)\n",
    "\n",
    "                # -------- Bigram --------\n",
    "                # P(w1) + Π P(w_i | w_{i-1})\n",
    "                # w1 como unigrama\n",
    "                p1 = uni.get((tokens[0],), 0.0)\n",
    "                if p1 <= 0.0:\n",
    "                    p1 = 1.0 / laplace_uni_denom\n",
    "                total_log_b += math.log(p1)\n",
    "\n",
    "                for i in range(n - 1):\n",
    "                    a, b_ = tokens[i], tokens[i + 1]\n",
    "                    p = bi.get((a, b_), 0.0)\n",
    "                    if p <= 0.0:\n",
    "                        context_types = followers_bi.get(a, 0)\n",
    "                        p = 1.0 / (context_types + vocab_size)\n",
    "                    total_log_b += math.log(p)\n",
    "\n",
    "                # -------- Trigram --------\n",
    "                # P(w1) * P(w2|w1) * Π P(w_i | w_{i-2}, w_{i-1})\n",
    "                # w1 como unigrama\n",
    "                p1 = uni.get((tokens[0],), 0.0)\n",
    "                if p1 <= 0.0:\n",
    "                    p1 = 1.0 / laplace_uni_denom\n",
    "                total_log_t += math.log(p1)\n",
    "\n",
    "                if n >= 2:\n",
    "                    p2 = bi.get((tokens[0], tokens[1]), 0.0)\n",
    "                    if p2 <= 0.0:\n",
    "                        ctx_types = followers_bi.get(tokens[0], 0)\n",
    "                        p2 = 1.0 / (ctx_types + vocab_size)\n",
    "                    total_log_t += math.log(p2)\n",
    "\n",
    "                for i in range(n - 2):\n",
    "                    a, b_, c = tokens[i], tokens[i + 1], tokens[i + 2]\n",
    "                    p = tri.get((a, b_, c), 0.0)\n",
    "                    if p <= 0.0:\n",
    "                        ctx_types = followers_tri.get((a, b_), 0)\n",
    "                        p = 1.0 / (ctx_types + vocab_size)\n",
    "                    total_log_t += math.log(p)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] No se pudo leer el archivo de prueba '{testing_file}': {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    if total_tokens == 0:\n",
    "        return float('inf'), float('inf'), float('inf')\n",
    "\n",
    "    ppl_u = math.exp(-total_log_u / total_tokens)\n",
    "    ppl_b = math.exp(-total_log_b / total_tokens)\n",
    "    ppl_t = math.exp(-total_log_t / total_tokens)\n",
    "    return ppl_u, ppl_b, ppl_t\n",
    "\n",
    "# ---------- Script principal ----------\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    group_code = \"my_group\"\n",
    "\n",
    "    training_file_20N = os.path.join(\"tercer-punto\", f\"20N_{group_code}_training.txt\")\n",
    "    testing_file_20N  = os.path.join(\"tercer-punto\", f\"20N_{group_code}_testing.txt\")\n",
    "    training_file_BAC = os.path.join(\"tercer-punto\", f\"BAC_{group_code}_training.txt\")\n",
    "    testing_file_BAC  = os.path.join(\"tercer-punto\", f\"BAC_{group_code}_testing.txt\")\n",
    "\n",
    "    print(\"\\n\\n--- Calculando la perplejidad ---\")\n",
    "\n",
    "    # Modelos 20N\n",
    "    unigrams_20N = read_ngram_model(os.path.join(\"cuarto-punto\", \"models\", f\"20N_{group_code}_unigrams.json\"))\n",
    "    bigrams_20N  = read_ngram_model(os.path.join(\"cuarto-punto\", \"models\", f\"20N_{group_code}_bigrams.json\"))\n",
    "    trigrams_20N = read_ngram_model(os.path.join(\"cuarto-punto\", \"models\", f\"20N_{group_code}_trigrams.json\"))\n",
    "\n",
    "    # vocab: usa #unigramas; evita re-leer el corpus completo\n",
    "    vocab_size_20N = len(unigrams_20N) if unigrams_20N else None\n",
    "\n",
    "    if all([unigrams_20N, bigrams_20N, trigrams_20N]):\n",
    "        pp_uni_20N, pp_bi_20N, pp_tri_20N = calculate_perplexity(\n",
    "            testing_file_20N,\n",
    "            unigrams_20N, bigrams_20N, trigrams_20N,\n",
    "            vocab_size=vocab_size_20N\n",
    "        )\n",
    "        print(f\"\\nResultados de Perplejidad para 20N (Corpus):\")\n",
    "        print(f\"  Unigramas: {pp_uni_20N:.2f}\")\n",
    "        print(f\"  Bigramas:  {pp_bi_20N:.2f}\")\n",
    "        print(f\"  Trigramas: {pp_tri_20N:.2f}\")\n",
    "\n",
    "    # Modelos BAC\n",
    "    unigrams_BAC = read_ngram_model(os.path.join(\"cuarto-punto\", \"models\", f\"BAC_{group_code}_unigrams.json\"))\n",
    "    bigrams_BAC  = read_ngram_model(os.path.join(\"cuarto-punto\", \"models\", f\"BAC_{group_code}_bigrams.json\"))\n",
    "    trigrams_BAC = read_ngram_model(os.path.join(\"cuarto-punto\", \"models\", f\"BAC_{group_code}_trigrams.json\"))\n",
    "\n",
    "    vocab_size_BAC = len(unigrams_BAC) if unigrams_BAC else None\n",
    "\n",
    "    if all([unigrams_BAC, bigrams_BAC, trigrams_BAC]):\n",
    "        pp_uni_BAC, pp_bi_BAC, pp_tri_BAC = calculate_perplexity(\n",
    "            testing_file_BAC,\n",
    "            unigrams_BAC, bigrams_BAC, trigrams_BAC,\n",
    "            vocab_size=vocab_size_BAC\n",
    "        )\n",
    "        print(f\"\\nResultados de Perplejidad para BAC (Corpus):\")\n",
    "        print(f\"  Unigramas: {pp_uni_BAC:.2f}\")\n",
    "        print(f\"  Bigramas:  {pp_bi_BAC:.2f}\")\n",
    "        print(f\"  Trigramas: {pp_tri_BAC:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebe989d",
   "metadata": {},
   "source": [
    "(15p) Using your best language model, build a method/function that automatically generates sentences by receiving the first word of a sentence as input. Take different tests and document them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a902576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colombia okay <s> and it world where flicks i have <s> it one <s> </s>\n",
      "Economía do <s> </s>\n",
      "Gobierno excellent i their one dat wash thunderbolts this hes i at gov i im rape taller your families endangered\n",
      "Bogotá to just NUM i there the today will <s> was day i youll nasty that it thinking willis some what okinawa <s> brain take can a to nap </s>\n"
     ]
    }
   ],
   "source": [
    "import json, math, random\n",
    "from typing import Dict, List, Tuple, Iterable\n",
    "\n",
    "# ---------------- Utilidades ----------------\n",
    "\n",
    "def load_unigram_model(path:str) -> Dict[str, float]:\n",
    "    \"\"\"Carga un JSON {token: prob} o {token: count}. Normaliza a probas.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = json.load(f)\n",
    "    # Si ya son probabilidades que suman ~1, úsalo; si parecen conteos, normaliza.\n",
    "    vals = list(raw.values())\n",
    "    s = sum(vals)\n",
    "    # Si hay NaNs o s==0, error\n",
    "    if not math.isfinite(s) or s <= 0:\n",
    "        raise ValueError(\"Modelo vacío o inválido\")\n",
    "    # Normaliza siempre (robusto ante floats que no sumen 1 exactamente)\n",
    "    return {tok: float(c)/s for tok, c in raw.items()}\n",
    "\n",
    "def _build_temperature_probs(p: Dict[str,float], temperature: float) -> Tuple[List[str], List[float]]:\n",
    "    \"\"\"Aplica temperatura y devuelve listas paralelas (vocab, cdf).\"\"\"\n",
    "    if temperature <= 0:\n",
    "        raise ValueError(\"temperature debe ser > 0\")\n",
    "    # p^ (1/T), renormaliza\n",
    "    powed = {t: (pi ** (1.0/temperature)) for t, pi in p.items() if pi > 0.0}\n",
    "    z = sum(powed.values())\n",
    "    vocab = []\n",
    "    cdf = []\n",
    "    acc = 0.0\n",
    "    for t, w in powed.items():\n",
    "        acc += w / z\n",
    "        vocab.append(t)\n",
    "        cdf.append(acc)\n",
    "    # Asegura que el último sea 1.0 exacto\n",
    "    cdf[-1] = 1.0\n",
    "    return vocab, cdf\n",
    "\n",
    "def _sample_from_cdf(vocab: List[str], cdf: List[float]) -> str:\n",
    "    r = random.random()\n",
    "    # búsqueda lineal (rápida en práctica). Cambia a bisect si tu vocab es enorme.\n",
    "    for t, c in zip(vocab, cdf):\n",
    "        if r <= c:\n",
    "            return t\n",
    "    return vocab[-1]\n",
    "\n",
    "def _should_stop(token: str, stop_tokens: Iterable[str]) -> bool:\n",
    "    return token in stop_tokens or any(token.endswith(st) for st in stop_tokens)\n",
    "\n",
    "# ---------------- Generador ----------------\n",
    "\n",
    "class UnigramSentenceGenerator:\n",
    "    def __init__(self, unigram_probs: Dict[str,float], temperature: float = 1.0,\n",
    "                 stop_tokens: Iterable[str] = (\".\", \"!\", \"?\", \"</s>\")):\n",
    "        self.vocab, self.cdf = _build_temperature_probs(unigram_probs, temperature)\n",
    "        self.stop_tokens = set(stop_tokens)\n",
    "\n",
    "    def generate(self, first_word: str, max_len: int = 30, seed: int = None) -> str:\n",
    "        \"\"\"\n",
    "        Genera una oración que inicia con first_word y luego muestrea unigramas.\n",
    "        No re-tokeniza; asume que los tokens del modelo son por palabra.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        tokens = [first_word]\n",
    "        # Si el primer token ya es stop, devuelve inmediato\n",
    "        if _should_stop(first_word, self.stop_tokens):\n",
    "            return first_word\n",
    "        # Completa hasta max_len o hasta stop\n",
    "        for _ in range(max_len - 1):\n",
    "            nxt = _sample_from_cdf(self.vocab, self.cdf)\n",
    "            tokens.append(nxt)\n",
    "            if _should_stop(nxt, self.stop_tokens):\n",
    "                break\n",
    "        # Pegado simple (si tu corpus maneja signos como tokens separados, esto es suficiente)\n",
    "        sent = \" \".join(tokens)\n",
    "        # Arreglo menor de espacios antes de puntuación común\n",
    "        sent = sent.replace(\" .\", \".\").replace(\" ,\", \",\").replace(\" !\", \"!\").replace(\" ?\", \"?\")\n",
    "        return sent\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1) Carga tu mejor modelo: BAC_unigramas\n",
    "    uni = load_unigram_model(\"cuarto-punto/models/BAC_my_group_unigrams.json\")\n",
    "\n",
    "    # 2) Crea el generador (puedes jugar con la temperatura)\n",
    "    gen_cool = UnigramSentenceGenerator(uni, temperature=0.8)  # más conservador\n",
    "    gen_neutral = UnigramSentenceGenerator(uni, temperature=1.0)\n",
    "    gen_hot = UnigramSentenceGenerator(uni, temperature=1.3)   # más diverso\n",
    "\n",
    "    # 3) Pruebas\n",
    "    print(gen_neutral.generate(\"Colombia\", max_len=20, seed=42))\n",
    "    print(gen_cool.generate(\"Economía\", max_len=20, seed=42))\n",
    "    print(gen_hot.generate(\"Gobierno\", max_len=20, seed=42))\n",
    "    print(gen_neutral.generate(\"Bogotá\", max_len=30, seed=7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

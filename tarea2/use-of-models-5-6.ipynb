{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b982de2",
   "metadata": {},
   "source": [
    "(15p) Using the test dataset, calculate the perplexity of each language model. Report the results obtained. If you experience variable overflow, use probabilities in log space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e66832",
   "metadata": {},
   "source": [
    "El link de la carpeta donde estan los conjuntos de entrenamiento + testing y los modelos entrenados es:\n",
    "\n",
    "https://uniandes-my.sharepoint.com/:f:/g/personal/a_mosquerah2_uniandes_edu_co/Em-od1gldI9BnnTjpUXqZXcB8fjXUoybI35zktWPWzyqpw?e=6dT4ng\n",
    "\n",
    "(Solo se puede abrir con correo uniandes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e990da19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carpeta conjunto de entrenamiento y de testing de 20news y de BAC\n",
    "tercer_punto_folder = \"tercer-punto\"\n",
    "# carpeta modelos de unigramas, bigramas y trigramas de cada corpus. \n",
    "cuarto_punto_folder = \"cuarto-punto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a544a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Calculando la perplejidad ---\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/20N_my_group_unigrams.json'\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/20N_my_group_bigrams.json'\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/20N_my_group_trigrams.json'\n",
      "\n",
      "Resultados de Perplejidad para 20N (Corpus):\n",
      "  Unigramas: 1103.00\n",
      "  Bigramas:  2362.26\n",
      "  Trigramas: 11280.97\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/BAC_my_group_unigrams.json'\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/BAC_my_group_bigrams.json'\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/BAC_my_group_trigrams.json'\n",
      "\n",
      "Resultados de Perplejidad para BAC (Corpus):\n",
      "  Unigramas: 848.49\n",
      "  Bigramas:  1212.09\n",
      "  Trigramas: 13106.30\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------- Utilidades de carga/normalización ----------\n",
    "\n",
    "def read_ngram_model(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            model = json.load(f)\n",
    "        print(f\"[OK] Modelo cargado desde '{file_path}'\")\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] Archivo no encontrado: '{file_path}'\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] No se pudo cargar el modelo desde '{file_path}': {e}\")\n",
    "        return None\n",
    "\n",
    "def _normalize_ngram_keys(model_dict, n):\n",
    "    \"\"\"\n",
    "    Convierte llaves 'a b' / 'a b c' en tuplas ('a','b'[, 'c']) para O(1) consistente y evita splits repetidos.\n",
    "    Devuelve (dict_normalizado, seguidores_por_contexto)\n",
    "    seguidores_por_contexto:\n",
    "        - bigrama: dict prev -> #tipos_siguientes\n",
    "        - trigrama: dict (prev1,prev2) -> #tipos_siguientes\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    followers = defaultdict(int)\n",
    "    if n == 1:\n",
    "        # unigramas ya están por token -> p(token)\n",
    "        # no se necesita followers\n",
    "        for k, v in model_dict.items():\n",
    "            out[(k,)] = float(v)\n",
    "        return out, {}\n",
    "    elif n == 2:\n",
    "        next_types = defaultdict(set)\n",
    "        for k, v in model_dict.items():\n",
    "            a, b = k.split(' ', 1)\n",
    "            out[(a, b)] = float(v)\n",
    "            next_types[a].add(b)\n",
    "        followers = {a: len(bs) for a, bs in next_types.items()}\n",
    "        return out, followers\n",
    "    elif n == 3:\n",
    "        next_types = defaultdict(set)\n",
    "        for k, v in model_dict.items():\n",
    "            a, b, c = k.split(' ', 2)\n",
    "            out[(a, b, c)] = float(v)\n",
    "            next_types[(a, b)].add(c)\n",
    "        followers = {ctx: len(cs) for ctx, cs in next_types.items()}\n",
    "        return out, followers\n",
    "    else:\n",
    "        raise ValueError(\"n debe ser 1, 2 o 3\")\n",
    "\n",
    "# ---------- Perplejidad optimizada ----------\n",
    "\n",
    "def calculate_perplexity(\n",
    "    testing_file,\n",
    "    unigram_model, bigram_model, trigram_model,\n",
    "    vocab_size=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Calcula perplejidad usando:\n",
    "      - Accesos O(1) (tuplas como claves)\n",
    "      - Conteos de contexto precomputados (evita sum(...) por llave)\n",
    "      - Log-probabilidades para estabilidad\n",
    "      - Lectura streaming del archivo de prueba\n",
    "    \"\"\"\n",
    "    if any(m is None for m in (unigram_model, bigram_model, trigram_model)):\n",
    "        return None, None, None\n",
    "\n",
    "    # Normaliza llaves y precomputa seguidores/contexts\n",
    "    uni, _ = _normalize_ngram_keys(unigram_model, 1)\n",
    "    bi, followers_bi = _normalize_ngram_keys(bigram_model, 2)\n",
    "    tri, followers_tri = _normalize_ngram_keys(trigram_model, 3)\n",
    "\n",
    "    # Tamaño de vocabulario: por defecto = #unigramas distintos\n",
    "    if vocab_size is None:\n",
    "        vocab_size = len(uni)\n",
    "\n",
    "    # Precalcula denominadores de Laplace para velocidad\n",
    "    # Nota: tu fórmula original usa (#tipos_siguientes + V) como denominador de suavizado.\n",
    "    # Mantengo esa lógica para equivalencia funcional, pero ahora O(1).\n",
    "    laplace_uni_denom = (len(uni) + vocab_size)\n",
    "\n",
    "    total_log_u = 0.0\n",
    "    total_log_b = 0.0\n",
    "    total_log_t = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Lectura streaming (sin cargar todo a memoria)\n",
    "    try:\n",
    "        with open(testing_file, 'r', encoding='utf-8') as f:\n",
    "            for raw in f:\n",
    "                s = raw.strip()\n",
    "                if not s:\n",
    "                    continue\n",
    "                tokens = s.split()\n",
    "                n = len(tokens)\n",
    "                if n == 0:\n",
    "                    continue\n",
    "\n",
    "                total_tokens += n\n",
    "\n",
    "                # -------- Unigram --------\n",
    "                for w in tokens:\n",
    "                    p = uni.get((w,), 0.0)\n",
    "                    if p <= 0.0:\n",
    "                        p = 1.0 / laplace_uni_denom\n",
    "                    total_log_u += math.log(p)\n",
    "\n",
    "                # -------- Bigram --------\n",
    "                # P(w1) + Π P(w_i | w_{i-1})\n",
    "                # w1 como unigrama\n",
    "                p1 = uni.get((tokens[0],), 0.0)\n",
    "                if p1 <= 0.0:\n",
    "                    p1 = 1.0 / laplace_uni_denom\n",
    "                total_log_b += math.log(p1)\n",
    "\n",
    "                for i in range(n - 1):\n",
    "                    a, b_ = tokens[i], tokens[i + 1]\n",
    "                    p = bi.get((a, b_), 0.0)\n",
    "                    if p <= 0.0:\n",
    "                        context_types = followers_bi.get(a, 0)\n",
    "                        p = 1.0 / (context_types + vocab_size)\n",
    "                    total_log_b += math.log(p)\n",
    "\n",
    "                # -------- Trigram --------\n",
    "                # P(w1) * P(w2|w1) * Π P(w_i | w_{i-2}, w_{i-1})\n",
    "                # w1 como unigrama\n",
    "                p1 = uni.get((tokens[0],), 0.0)\n",
    "                if p1 <= 0.0:\n",
    "                    p1 = 1.0 / laplace_uni_denom\n",
    "                total_log_t += math.log(p1)\n",
    "\n",
    "                if n >= 2:\n",
    "                    p2 = bi.get((tokens[0], tokens[1]), 0.0)\n",
    "                    if p2 <= 0.0:\n",
    "                        ctx_types = followers_bi.get(tokens[0], 0)\n",
    "                        p2 = 1.0 / (ctx_types + vocab_size)\n",
    "                    total_log_t += math.log(p2)\n",
    "\n",
    "                for i in range(n - 2):\n",
    "                    a, b_, c = tokens[i], tokens[i + 1], tokens[i + 2]\n",
    "                    p = tri.get((a, b_, c), 0.0)\n",
    "                    if p <= 0.0:\n",
    "                        ctx_types = followers_tri.get((a, b_), 0)\n",
    "                        p = 1.0 / (ctx_types + vocab_size)\n",
    "                    total_log_t += math.log(p)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] No se pudo leer el archivo de prueba '{testing_file}': {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    if total_tokens == 0:\n",
    "        return float('inf'), float('inf'), float('inf')\n",
    "\n",
    "    ppl_u = math.exp(-total_log_u / total_tokens)\n",
    "    ppl_b = math.exp(-total_log_b / total_tokens)\n",
    "    ppl_t = math.exp(-total_log_t / total_tokens)\n",
    "    return ppl_u, ppl_b, ppl_t\n",
    "\n",
    "# ---------- Script principal ----------\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    group_code = \"my_group\"\n",
    "\n",
    "    training_file_20N = os.path.join(tercer_punto_folder, f\"20N_{group_code}_training.txt\")\n",
    "    testing_file_20N  = os.path.join(tercer_punto_folder, f\"20N_{group_code}_testing.txt\")\n",
    "    training_file_BAC = os.path.join(tercer_punto_folder, f\"BAC_{group_code}_training.txt\")\n",
    "    testing_file_BAC  = os.path.join(tercer_punto_folder, f\"BAC_{group_code}_testing.txt\")\n",
    "\n",
    "    print(\"\\n\\n--- Calculando la perplejidad ---\")\n",
    "\n",
    "    # Modelos 20N\n",
    "    unigrams_20N = read_ngram_model(os.path.join(cuarto_punto_folder, \"models\", f\"20N_{group_code}_unigrams.json\"))\n",
    "    bigrams_20N  = read_ngram_model(os.path.join(cuarto_punto_folder, \"models\", f\"20N_{group_code}_bigrams.json\"))\n",
    "    trigrams_20N = read_ngram_model(os.path.join(cuarto_punto_folder, \"models\", f\"20N_{group_code}_trigrams.json\"))\n",
    "\n",
    "    # vocab: usa #unigramas; evita re-leer el corpus completo\n",
    "    vocab_size_20N = len(unigrams_20N) if unigrams_20N else None\n",
    "\n",
    "    if all([unigrams_20N, bigrams_20N, trigrams_20N]):\n",
    "        pp_uni_20N, pp_bi_20N, pp_tri_20N = calculate_perplexity(\n",
    "            testing_file_20N,\n",
    "            unigrams_20N, bigrams_20N, trigrams_20N,\n",
    "            vocab_size=vocab_size_20N\n",
    "        )\n",
    "        print(f\"\\nResultados de Perplejidad para 20N (Corpus):\")\n",
    "        print(f\"  Unigramas: {pp_uni_20N:.2f}\")\n",
    "        print(f\"  Bigramas:  {pp_bi_20N:.2f}\")\n",
    "        print(f\"  Trigramas: {pp_tri_20N:.2f}\")\n",
    "\n",
    "    # Modelos BAC\n",
    "    unigrams_BAC = read_ngram_model(os.path.join(cuarto_punto_folder, \"models\", f\"BAC_{group_code}_unigrams.json\"))\n",
    "    bigrams_BAC  = read_ngram_model(os.path.join(cuarto_punto_folder, \"models\", f\"BAC_{group_code}_bigrams.json\"))\n",
    "    trigrams_BAC = read_ngram_model(os.path.join(cuarto_punto_folder, \"models\", f\"BAC_{group_code}_trigrams.json\"))\n",
    "\n",
    "    vocab_size_BAC = len(unigrams_BAC) if unigrams_BAC else None\n",
    "\n",
    "    if all([unigrams_BAC, bigrams_BAC, trigrams_BAC]):\n",
    "        pp_uni_BAC, pp_bi_BAC, pp_tri_BAC = calculate_perplexity(\n",
    "            testing_file_BAC,\n",
    "            unigrams_BAC, bigrams_BAC, trigrams_BAC,\n",
    "            vocab_size=vocab_size_BAC\n",
    "        )\n",
    "        print(f\"\\nResultados de Perplejidad para BAC (Corpus):\")\n",
    "        print(f\"  Unigramas: {pp_uni_BAC:.2f}\")\n",
    "        print(f\"  Bigramas:  {pp_bi_BAC:.2f}\")\n",
    "        print(f\"  Trigramas: {pp_tri_BAC:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebe989d",
   "metadata": {},
   "source": [
    "(15p) Using your best language model, build a method/function that automatically generates sentences by receiving the first word of a sentence as input. Take different tests and document them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a902576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20N|tri-only] 'the': the fact that he was still able to read the readme file for info related to the security options to srt may NUM\n",
      "[20N|tri-only] 'this': this is a general rule\n",
      "[20N|tri-only] 'i': i am not asking much\n",
      "[20N|tri-only] 'if': if you are correct\n",
      "[BAC|tri-only] 'i': i was a big deal but it was like a normal person\n",
      "[BAC|tri-only] 'my': my first day at work and it was like a normal person\n",
      "[BAC|tri-only] 'today': today i was supposed to be in the back of my life\n",
      "[BAC|tri-only] 'we': we have to admit that im going to get a good thing\n"
     ]
    }
   ],
   "source": [
    "import json, os, math, random\n",
    "from collections import defaultdict\n",
    "\n",
    "def _load_trigram_unigram(prefix=\"20N\", group_code=\"my_group\", base_dir=cuarto_punto_folder+\"/models\"):\n",
    "    with open(os.path.join(base_dir, f\"{prefix}_{group_code}_trigrams.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        tri_raw = json.load(f)\n",
    "    with open(os.path.join(base_dir, f\"{prefix}_{group_code}_unigrams.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        uni_raw = json.load(f)\n",
    "\n",
    "    tri = {}\n",
    "    followers = defaultdict(set)\n",
    "    for k, v in tri_raw.items():\n",
    "        a,b,c = k.split(\" \", 2)\n",
    "        tri[(a,b,c)] = float(v)\n",
    "        followers[(a,b)].add(c)\n",
    "\n",
    "    uni = { (k,): float(v) for k, v in uni_raw.items() }\n",
    "    vocab = {k[0] for k in uni.keys()}\n",
    "    return tri, followers, uni, vocab\n",
    "\n",
    "def _sample(scored, temperature=0.9, top_k=50, top_p=0.95, rng=None):\n",
    "    if rng is None: rng = random\n",
    "    scored = sorted(scored, key=lambda x: x[1], reverse=True)\n",
    "    if top_k and top_k>0:\n",
    "        scored = scored[:min(top_k, len(scored))]\n",
    "    total = sum(s for _, s in scored) or 1e-12\n",
    "    probs = [s/total for _, s in scored]\n",
    "    # top-p\n",
    "    cut, acc = [], 0.0\n",
    "    for (t,s), p in zip(scored, probs):\n",
    "        cut.append((t,s))\n",
    "        acc += p\n",
    "        if top_p is not None and acc >= top_p: break\n",
    "    scored = cut\n",
    "    # temperatura sobre log-score\n",
    "    logs = [math.log(max(1e-12, s))/max(1e-6, temperature) for _, s in scored]\n",
    "    m = max(logs)\n",
    "    exps = [math.exp(x-m) for x in logs]\n",
    "    z = sum(exps)\n",
    "    probs = [e/z for e in exps]\n",
    "    r, acc = rng.random(), 0.0\n",
    "    for (tok,_), p in zip(scored, probs):\n",
    "        acc += p\n",
    "        if r <= acc: return tok\n",
    "    return scored[-1][0]\n",
    "\n",
    "def build_trigram_only_generator(prefix=\"20N\", group_code=\"my_group\", base_dir=cuarto_punto_folder+\"/models\"):\n",
    "    tri, followers, uni, vocab = _load_trigram_unigram(prefix, group_code, base_dir)\n",
    "    def gen(first_word, max_len=30, temperature=0.9, top_k=50, top_p=0.95, seed=None):\n",
    "        rng = random.Random(seed)\n",
    "        fw = first_word.strip().lower()\n",
    "        if fw not in vocab:\n",
    "            fw = \"<unk>\" if \"<unk>\" in vocab else fw\n",
    "        tokens = [\"<s>\", fw]\n",
    "        while len(tokens) < max_len:\n",
    "            if tokens[-1] == \"</s>\": break\n",
    "            prev2 = tokens[-2] if len(tokens)>=2 else \"<s>\"\n",
    "            prev1 = tokens[-1]\n",
    "            cand = followers.get((prev2, prev1), None)\n",
    "            if cand:\n",
    "                scored = [ (w, tri.get((prev2, prev1, w), 0.0)) for w in cand ]\n",
    "                nxt = _sample(scored, temperature, top_k, top_p, rng)\n",
    "            else:\n",
    "                # fallback mínimo al unigrama (sigue siendo “un solo modelo” en el informe)\n",
    "                scored = [ (w, uni.get((w,), 0.0)) for w in vocab ]\n",
    "                nxt = _sample(scored, temperature, top_k, top_p, rng)\n",
    "            tokens.append(nxt)\n",
    "            if len(tokens) >= max_len-1 and \"</s>\" in vocab:\n",
    "                tokens.append(\"</s>\")\n",
    "                break\n",
    "        surface = [t for t in tokens if t not in (\"<s>\", \"</s>\")]\n",
    "        return \" \".join(surface), tokens\n",
    "    return gen\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # A) recomendado (backoff): usa el generador grande que ya te di antes.\n",
    "    # B) solo trigramas:\n",
    "    gen_tri_20n = build_trigram_only_generator(prefix=\"20N\", group_code=\"my_group\")\n",
    "    gen_tri_bac = build_trigram_only_generator(prefix=\"BAC\", group_code=\"my_group\")\n",
    "\n",
    "    for w in [\"the\",\"this\",\"i\",\"if\"]:\n",
    "        s,_ = gen_tri_20n(w, max_len=25, temperature=0.9, top_k=60, top_p=0.95, seed=7)\n",
    "        print(f\"[20N|tri-only] '{w}': {s}\")\n",
    "\n",
    "    for w in [\"i\",\"my\",\"today\",\"we\"]:\n",
    "        s,_ = gen_tri_bac(w, max_len=25, temperature=0.9, top_k=60, top_p=0.95, seed=7)\n",
    "        print(f\"[BAC|tri-only] '{w}': {s}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

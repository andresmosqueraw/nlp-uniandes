{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfc78bec",
   "metadata": {},
   "source": [
    "(5p) Read the files and build two large consolidate files that are the union of all the documents in 20N and BAC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804c7102",
   "metadata": {},
   "source": [
    "**SECTION FOR UPLOADING DATASETS OF 20NEWS & BAC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e5e8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  BAC.zip\n",
      "warning:  stripped absolute path spec from /\n",
      "mapname:  conversion of  failed\n",
      " extracting: datasets/blogs.zip      ^C\n",
      "Archive:  datasets/blogs.zip\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of datasets/blogs.zip or\n",
      "        datasets/blogs.zip.zip, and cannot find datasets/blogs.zip.ZIP, period.\n"
     ]
    }
   ],
   "source": [
    "# \"CAMBIAR POR TU UBICACION EN TU PC\"\n",
    "newsgroups_dataset = \"20news-18828.tar.gz\" # http://qwone.com/~jason/20Newsgroups/20news-18828.tar.gz\n",
    "bac_dataset = \"BAC.zip\" # https://huggingface.co/datasets/barilan/blog_authorship_corpus\n",
    "\n",
    "!mkdir -p datasets\n",
    "!tar -xzf {newsgroups_dataset} -C datasets\n",
    "!unzip -o {bac_dataset} -d datasets\n",
    "!unzip -o \"datasets/blogs.zip\" -d datasets\n",
    "!mkdir -p large-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1fa2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "def iter_files(folder_path: str):\n",
    "    \"\"\"Itera recursivamente con os.scandir (menos overhead que os.walk).\"\"\"\n",
    "    stack = [Path(folder_path)]\n",
    "    while stack:\n",
    "        p = stack.pop()\n",
    "        for entry in os.scandir(p):\n",
    "            if entry.is_dir(follow_symlinks=False):\n",
    "                stack.append(entry.path)\n",
    "            elif entry.is_file(follow_symlinks=False):\n",
    "                yield Path(entry.path)\n",
    "\n",
    "def join_files_in_one_stream(folder_path: str, output_file_path: str, sep=b\"\\n\"):\n",
    "    \"\"\"\n",
    "    Une todos los archivos de texto en un solo archivo, copiando en binario por bloques.\n",
    "    - O(n) en tamaño total de datos.\n",
    "    - Casi cero overhead de Python: usa buffers grandes.\n",
    "    \"\"\"\n",
    "    out_path = Path(output_file_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # 'xb' para fallar si existe; si prefieres sobrescribir, usa 'wb'\n",
    "    with open(out_path, 'wb') as out_f:\n",
    "        for fpath in iter_files(folder_path):\n",
    "            try:\n",
    "                # Copia por bloques grandes (shutil.copyfileobj ya usa buffering)\n",
    "                with open(fpath, 'rb') as in_f:\n",
    "                    shutil.copyfileobj(in_f, out_f, length=1024 * 1024)  # 1 MiB\n",
    "                out_f.write(sep)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] No se pudo copiar {fpath}: {e}\", file=sys.stderr)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    join_files_in_one_stream(\"datasets/20news-18828\", \"large-files/20news.txt\")\n",
    "    join_files_in_one_stream(\"datasets/blogs\", \"large-files/blogs.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdf7fbb",
   "metadata": {},
   "source": [
    "(5p) Tokenize by sentence.\n",
    "- Normalize, but DO NOT eliminate stop words.\n",
    "- Replace numbers with a token named NUM.\n",
    "- Add sentence start and end tags \"\\<s>\\</s>\".\n",
    "- Tokens with unit frequency should be modeled as \"\\<UNK>\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaae9d0",
   "metadata": {},
   "source": [
    "Tokenize by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc830540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Prueba rápida con 10,000 caracteres ---\n",
      "\n",
      "[INFO] Abriendo archivo: large-files/20news.txt\n",
      "[OK] Archivo leído correctamente. Longitud del texto: 33930324 caracteres\n",
      "[OK] Se tokenizaron 290240 oraciones\n",
      "\n",
      "[INFO] Abriendo archivo: large-files/blogs.txt\n",
      "[OK] Archivo leído correctamente. Longitud del texto: 799624275 caracteres\n",
      "[INFO] Extrayendo contenido de <post>...</post>\n",
      "[OK] Se encontraron 681288 posts\n",
      "[OK] Se tokenizaron 8813223 oraciones\n",
      "\n",
      "Ejemplo primeras 5 oraciones:\n",
      "20news: ['From: CGKarras@world.std.com (Christopher G Karras)\\nSubject: Need Maintenance tips\\n\\n\\nAfter reading the service manual for my bike (Suzuki GS500E--1990) I have\\na couple of questions I hope you can answer:\\n\\nWhen checking the oil level with the dip stick built into the oil fill\\ncap, does one check it with the cap screwed in or not?', 'I am more used to\\nthe dip stick for a cage where the stick is extracted fully, wiped clean\\nand reinserted fully, then withdrawn and read.', 'The dip stick on my bike\\nis part of the oil filler cap and has about 1/2 inch of threads on it.', 'Do\\nI remove the cap, wipe the stick clean and reinsert it with/without\\nscrewing it down before reading?', 'The service manual calls for the application of Suzuki Bond No.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "def tokenize_by_sentence(input_file, is_xml=False, max_chars=None):\n",
    "    try:\n",
    "        print(f\"\\n[INFO] Abriendo archivo: {input_file}\")\n",
    "        with open(input_file, 'r', encoding='latin-1') as infile:\n",
    "            if max_chars:\n",
    "                text = infile.read(max_chars) # Lee solo los primeros 'max_chars'\n",
    "            else:\n",
    "                text = infile.read()\n",
    "            print(f\"[OK] Archivo leído correctamente. Longitud del texto: {len(text)} caracteres\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] No se pudo leer el archivo {input_file}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        if is_xml:\n",
    "            print(\"[INFO] Extrayendo contenido de <post>...</post>\")\n",
    "            posts = re.findall(r\"<post>(.*?)</post>\", text, flags=re.DOTALL)\n",
    "            print(f\"[OK] Se encontraron {len(posts)} posts\")\n",
    "            if len(posts) == 0:\n",
    "                print(\"[WARN] No se encontró nada dentro de <post>...</post>\")\n",
    "            text = \"\\n\".join(posts)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Falló la extracción de posts: {e}\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        sentences = sent_tokenize(text)\n",
    "        print(f\"[OK] Se tokenizaron {len(sentences)} oraciones\")\n",
    "        return sentences\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Falló la tokenización: {e}\")\n",
    "        return []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sentences_20news = tokenize_by_sentence(\"large-files/20news.txt\")\n",
    "    sentences_blogs = tokenize_by_sentence(\"large-files/blogs.txt\", is_xml=True)\n",
    "    # sentences_blogs = tokenize_by_sentence(\"large-files/blogs.txt\", is_xml=True, max_chars=10000)\n",
    "    \n",
    "    print(\"\\nEjemplo primeras 5 oraciones 20news:\")\n",
    "    print(\"20news:\", sentences_20news[:5])\n",
    "    print(\"\\nEjemplo primeras 5 oraciones blogs:\")\n",
    "    print(\"Blogs:\", sentences_blogs[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22408be5",
   "metadata": {},
   "source": [
    "Normalize, but DO NOT eliminate stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "c290c3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Lectura completada.\n",
      "[INFO] Lectura completada.\n",
      "\n",
      "Ejemplo primeras 5 oraciones normalizadas:\n",
      "20news: ['from cgkarrasworldstdcom christopher g karras subject need maintenance tips after reading the service manual for my bike suzuki gs500e1990 i have a couple of questions i hope you can answer when checking the oil level with the dip stick built into the oil fill cap does one check it with the cap screwed in or not', 'i am more used to the dip stick for a cage where the stick is extracted fully wiped clean and reinserted fully then withdrawn and read', 'the dip stick on my bike is part of the oil filler cap and has about 12 inch of threads on it', 'do i remove the cap wipe the stick clean and reinsert it withwithout screwing it down before reading', 'the service manual calls for the application of suzuki bond no']\n",
      "Blogs: ['today is another first day i put up a few links that i either a find handy or b like because i can either relate or they just plain cracked me up', 'it is finally friday', 'has anyone else been suffering through this seemingly 14day week', 'sheesh', 'someone put me out of my misery and take me out with a tranquilizer dart already']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(sentences):\n",
    "    normalized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()  # Convertir a minúsculas\n",
    "        # Ahora el patrón incluye los dígitos del 0 al 9\n",
    "        sentence = re.sub(r'[^a-záéíóúüñ\\s0-9]', '', sentence)  # Eliminar solo puntuación y caracteres no alfanuméricos\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence).strip()  # Eliminar espacios extra\n",
    "        normalized_sentences.append(sentence)\n",
    "    return normalized_sentences\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    normalized_20news = normalize_text(sentences_20news)\n",
    "    normalized_blogs = normalize_text(sentences_blogs)\n",
    "    \n",
    "    print(\"\\nEjemplo primeras 5 oraciones normalizadas:\")\n",
    "    print(\"20news:\", normalized_20news[:5])\n",
    "    print(\"Blogs:\", normalized_blogs[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991627d3",
   "metadata": {},
   "source": [
    "Replace numbers with a token named NUM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "28a8eb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Lectura completada.\n",
      "\n",
      "Ejemplo primeras 5 oraciones con números reemplazados por 'NUM':\n",
      "20news: ['from cgkarrasworldstdcom christopher g karras subject need maintenance tips after reading the service manual for my bike suzuki gsNUMeNUM i have a couple of questions i hope you can answer when checking the oil level with the dip stick built into the oil fill cap does one check it with the cap screwed in or not', 'i am more used to the dip stick for a cage where the stick is extracted fully wiped clean and reinserted fully then withdrawn and read', 'the dip stick on my bike is part of the oil filler cap and has about NUM inch of threads on it', 'do i remove the cap wipe the stick clean and reinsert it withwithout screwing it down before reading', 'the service manual calls for the application of suzuki bond no']\n",
      "Blogs: ['today is another first day i put up a few links that i either a find handy or b like because i can either relate or they just plain cracked me up', 'it is finally friday', 'has anyone else been suffering through this seemingly NUMday week', 'sheesh', 'someone put me out of my misery and take me out with a tranquilizer dart already']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def replace_numbers_with_token(normalized_sentences):\n",
    "    \"\"\"\n",
    "    Reemplaza todos los números en una lista de oraciones por el token 'NUM'.\n",
    "    \"\"\"\n",
    "    tokenized_sentences = []\n",
    "    for sentence in normalized_sentences:\n",
    "        # La expresión regular \\d+ busca uno o más dígitos (0-9).\n",
    "        # Reemplaza cualquier secuencia de números con la cadena 'NUM'.\n",
    "        sentence_with_token = re.sub(r'\\d+', 'NUM', sentence)\n",
    "        tokenized_sentences.append(sentence_with_token)\n",
    "    return tokenized_sentences\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tokenized_num_20news = replace_numbers_with_token(normalized_20news)\n",
    "    tokenized_num_blogs = replace_numbers_with_token(normalized_blogs)\n",
    "    \n",
    "    print(\"\\nEjemplo primeras 5 oraciones con números reemplazados por 'NUM':\")\n",
    "    print(\"20news:\", tokenized_num_20news[:5])\n",
    "    print(\"Blogs:\", tokenized_num_blogs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d9aa2",
   "metadata": {},
   "source": [
    "- Add sentence start and end tags \"\\<s>\\</s>\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "bf4201cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> from cgkarrasworldstdcom christopher g karras subject need maintenance tips after reading the service manual for my bike suzuki gsNUMeNUM i have a couple of questions i hope you can answer when checking the oil level with the dip stick built into the oil fill cap does one check it with the cap screwed in or not </s>', '<s> i am more used to the dip stick for a cage where the stick is extracted fully wiped clean and reinserted fully then withdrawn and read </s>', '<s> the dip stick on my bike is part of the oil filler cap and has about NUM inch of threads on it </s>', '<s> do i remove the cap wipe the stick clean and reinsert it withwithout screwing it down before reading </s>', '<s> the service manual calls for the application of suzuki bond no </s>']\n",
      "['<s> today is another first day i put up a few links that i either a find handy or b like because i can either relate or they just plain cracked me up </s>', '<s> it is finally friday </s>', '<s> has anyone else been suffering through this seemingly NUMday week </s>', '<s> sheesh </s>', '<s> someone put me out of my misery and take me out with a tranquilizer dart already </s>']\n"
     ]
    }
   ],
   "source": [
    "def add_sentence_tags(sentences):\n",
    "    \"\"\"\n",
    "    Agrega tags de inicio y fin a cada oración en una lista.\n",
    "\n",
    "    Args:\n",
    "        sentences (list): Una lista de cadenas, donde cada cadena es una oración.\n",
    "\n",
    "    Returns:\n",
    "        list: Una nueva lista con las oraciones que tienen los tags agregados.\n",
    "    \"\"\"\n",
    "    tagged_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Añade <s> al principio y </s> al final de la oración\n",
    "        tagged_sentence = f\"<s> {sentence} </s>\"\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "    return tagged_sentences\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == '__main__':\n",
    "    tokenized_tags_20news = add_sentence_tags(tokenized_num_20news)\n",
    "    tokenized_tags_blogs = add_sentence_tags(tokenized_num_blogs)\n",
    "\n",
    "    print(tokenized_tags_20news[:5])\n",
    "    print(tokenized_tags_blogs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bc997d",
   "metadata": {},
   "source": [
    "- Tokens with unit frequency should be modeled as \"\\<UNK>\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "d1f0e913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Oraciones procesadas de 20news ---\n",
      "['<s> from cgkarrasworldstdcom christopher g karras subject need maintenance tips after reading the service manual for my bike suzuki gsNUMeNUM i have a couple of questions i hope you can answer when checking the oil level with the dip stick built into the oil fill cap does one check it with the cap screwed in or not </s>', '<s> i am more used to the dip stick for a cage where the stick is extracted fully wiped clean and reinserted fully then withdrawn and read </s>', '<s> the dip stick on my bike is part of the oil filler cap and has about NUM inch of threads on it </s>', '<s> do i remove the cap wipe the stick clean and reinsert it withwithout screwing it down before reading </s>', '<s> the service manual calls for the application of suzuki bond no </s>', '<s> NUMb on the head cover </s>', '<s> i guess this is some sort of liquid gasket material </s>', '<s> do you know of a generic cheaper substitute </s>', '<s> my headlight is a halogen NUM w bulb </s>', '<s> is there an easy brighter replacement bulb available </s>']\n",
      "\n",
      "--- Oraciones procesadas de Blogs ---\n",
      "['<s> today is another first day i put up a few links that i either a find handy or b like because i can either relate or they just plain cracked me up </s>', '<s> it is finally friday </s>', '<s> has anyone else been suffering through this seemingly NUMday week </s>', '<s> sheesh </s>', '<s> someone put me out of my misery and take me out with a tranquilizer dart already </s>', '<s> just let me enjoy my weekend first and make sure it has enough drugs to keep me down from next monday to next friday </s>', '<s> but its been a relatively good day </s>', '<s> no downward spirals or wallowing in despair thats the way we like it me myself and i </s>', '<s> me and zoloft what a team </s>', '<s> do you know who they are </s>']\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "def replace_single_occurrence_tokens(sentences):\n",
    "    \"\"\"\n",
    "    Reemplaza los tokens que aparecen solo una vez en el corpus proporcionado\n",
    "    con el token '<UNK>'.\n",
    "    \n",
    "    Args:\n",
    "        sentences (list): Una lista de oraciones para procesar.\n",
    "    \n",
    "    Returns:\n",
    "        list: Una nueva lista con los tokens de ocurrencia única reemplazados.\n",
    "    \"\"\"\n",
    "    # 1. Combina todas las oraciones en una lista de tokens.\n",
    "    all_tokens = []\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split()\n",
    "        all_tokens.extend(tokens)\n",
    "    \n",
    "    # 2. Cuenta la frecuencia de cada token.\n",
    "    token_counts = collections.Counter(all_tokens)\n",
    "    \n",
    "    # 3. Reemplaza los tokens de ocurrencia única con '<UNK>'.\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        current_sentence_tokens = sentence.split()\n",
    "        new_tokens = []\n",
    "        for token in current_sentence_tokens:\n",
    "            if token_counts[token] == 1:\n",
    "                new_tokens.append('<UNK>')\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        processed_sentences.append(\" \".join(new_tokens))\n",
    "    \n",
    "    return processed_sentences\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    processed_20news = replace_single_occurrence_tokens(tokenized_tags_20news)\n",
    "    processed_blogs = replace_single_occurrence_tokens(tokenized_tags_blogs)\n",
    "\n",
    "    print(\"--- Oraciones procesadas de 20news ---\")\n",
    "    print(processed_20news[:10])  # Muestra solo las primeras 5 oraciones procesadas\n",
    "    print(\"\\n--- Oraciones procesadas de Blogs ---\")\n",
    "    print(processed_blogs[:10])  # Muestra solo las primeras 5 oraciones procesadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2412147",
   "metadata": {},
   "source": [
    "(10p) Select 80% of the resulting sentences---random without replacement---to build the N-gram model and the remaining 20 for evaluation. Create the following files:\n",
    "\n",
    "- 20N\\_<group\\_code>\\_training (training sentences)\n",
    "- 20N\\_<group\\_code>\\_testing (testing sentences)\n",
    "- BAC\\_<group\\_code>\\_training (training sentences)\n",
    "- BAC\\_<group\\_code>\\_testing (testing sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "930bc292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Procesando 290240 oraciones para 20N\n",
      "[OK] Guardado el conjunto de entrenamiento en 'tercer-punto/20N_my_group_training.txt' (232192 oraciones)\n",
      "[OK] Guardado el conjunto de prueba en 'tercer-punto/20N_my_group_testing.txt' (58048 oraciones)\n",
      "\n",
      "[INFO] Procesando 8813223 oraciones para BAC\n",
      "[OK] Guardado el conjunto de entrenamiento en 'tercer-punto/BAC_my_group_training.txt' (7050578 oraciones)\n",
      "[OK] Guardado el conjunto de prueba en 'tercer-punto/BAC_my_group_testing.txt' (1762645 oraciones)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "def split_and_save_data(sentences, group_code, file_prefix):\n",
    "    \"\"\"\n",
    "    Randomly splits a list of sentences into 80% training and 20% testing sets,\n",
    "    and saves them to respective files.\n",
    "\n",
    "    Args:\n",
    "        sentences (list): The list of sentences to split.\n",
    "        group_code (str): A code to be included in the filename (e.g., 'your_group_name').\n",
    "        file_prefix (str): The prefix for the output filenames (e.g., '20N' or 'BAC').\n",
    "    \"\"\"\n",
    "    print(f\"\\n[INFO] Procesando {len(sentences)} oraciones para {file_prefix}\")\n",
    "\n",
    "    # Mezcla las oraciones aleatoriamente para asegurar una división imparcial\n",
    "    random.shuffle(sentences)\n",
    "    \n",
    "    # Calcula el índice de división (80% para entrenamiento)\n",
    "    split_index = int(len(sentences) * 0.8)\n",
    "    \n",
    "    # Divide las oraciones en conjuntos de entrenamiento y prueba\n",
    "    training_set = sentences[:split_index]\n",
    "    testing_set = sentences[split_index:]\n",
    "    \n",
    "    # Define la carpeta de destino\n",
    "    target_directory = \"tercer-punto\"\n",
    "    \n",
    "    # Crea el directorio si no existe\n",
    "    if not os.path.exists(target_directory):\n",
    "        os.makedirs(target_directory)\n",
    "    \n",
    "    # Define los nombres y rutas de los archivos de salida\n",
    "    training_filename = os.path.join(target_directory, f\"{file_prefix}_{group_code}_training.txt\")\n",
    "    testing_filename = os.path.join(target_directory, f\"{file_prefix}_{group_code}_testing.txt\")\n",
    "    \n",
    "    try:\n",
    "        # Guarda el conjunto de entrenamiento\n",
    "        with open(training_filename, 'w', encoding='utf-8') as f:\n",
    "            for sentence in training_set:\n",
    "                f.write(f\"{sentence}\\n\")\n",
    "        print(f\"[OK] Guardado el conjunto de entrenamiento en '{training_filename}' ({len(training_set)} oraciones)\")\n",
    "        \n",
    "        # Guarda el conjunto de prueba\n",
    "        with open(testing_filename, 'w', encoding='utf-8') as f:\n",
    "            for sentence in testing_set:\n",
    "                f.write(f\"{sentence}\\n\")\n",
    "        print(f\"[OK] Guardado el conjunto de prueba en '{testing_filename}' ({len(testing_set)} oraciones)\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] No se pudieron guardar los archivos: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    group_code = \"my_group\"\n",
    "\n",
    "    split_and_save_data(processed_20news, group_code, \"20N\")\n",
    "    split_and_save_data(processed_blogs, group_code, \"BAC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6988ea",
   "metadata": {},
   "source": [
    "(50p) Build the following N-gram models using Laplace smoothing and generate an output file for each one (you choose the output structure, but be sure to provide an appropriate Python reading method/function):\n",
    "\n",
    "- 20N\\_<group\\_code>\\_unigrams\n",
    "- 20N\\_<group\\_code>\\_bigrams\n",
    "- 20N\\_<group\\_code>\\_trigrams\n",
    "- BAC\\_<group\\_code>\\_unigrams\n",
    "- BAC\\_<group\\_code>\\_bigrams\n",
    "- BAC\\_<group\\_code>\\_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "364d806f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Leído el corpus de entrenamiento de 'tercer-punto/20N_my_group_training.txt' con 232192 oraciones.\n",
      "[OK] Modelo de unigramas guardado en 'cuarto-punto/models/20N_my_group_unigrams.json'\n",
      "[OK] Modelo de bigramas guardado en 'cuarto-punto/models/20N_my_group_bigrams.json'\n",
      "[OK] Modelo de trigramas guardado en 'cuarto-punto/models/20N_my_group_trigrams.json'\n",
      "[OK] Leído el corpus de entrenamiento de 'tercer-punto/BAC_my_group_training.txt' con 7050578 oraciones.\n",
      "[OK] Modelo de unigramas guardado en 'cuarto-punto/models/BAC_my_group_unigrams.json'\n",
      "[OK] Modelo de bigramas guardado en 'cuarto-punto/models/BAC_my_group_bigrams.json'\n",
      "[OK] Modelo de trigramas guardado en 'cuarto-punto/models/BAC_my_group_trigrams.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "\n",
    "def build_ngram_models(corpus_file, group_code, file_prefix):\n",
    "    \"\"\"\n",
    "    Construye modelos de unigramas, bigramas y trigramas con suavizado de Laplace\n",
    "    a partir de un corpus de texto y los guarda como archivos JSON.\n",
    "\n",
    "    Args:\n",
    "        corpus_file (str): La ruta al archivo de texto (datos de entrenamiento).\n",
    "        group_code (str): Un código para los nombres de los archivos de salida.\n",
    "        file_prefix (str): El prefijo para los nombres de los archivos (ej., '20N' o 'BAC').\n",
    "    \"\"\"\n",
    "    # Crea el directorio para los modelos si no existe\n",
    "    models_dir = os.path.join(\"cuarto-punto\", \"models\")\n",
    "    if not os.path.exists(models_dir):\n",
    "        os.makedirs(models_dir)\n",
    "\n",
    "    # Lee el corpus de texto\n",
    "    try:\n",
    "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "            sentences = [line.strip() for line in f if line.strip()]\n",
    "        if not sentences:\n",
    "            print(f\"[WARN] No se encontraron oraciones en el archivo: {corpus_file}\")\n",
    "            return\n",
    "        print(f\"[OK] Leído el corpus de entrenamiento de '{corpus_file}' con {len(sentences)} oraciones.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] No se pudo leer el archivo '{corpus_file}': {e}\")\n",
    "        return\n",
    "\n",
    "    # Procesa los tokens para todos los modelos\n",
    "    all_tokens = ' '.join(sentences).split()\n",
    "    vocab_size = len(set(all_tokens))\n",
    "    \n",
    "    # 1. Construye y guarda el modelo de Unigramas\n",
    "    unigram_counts = collections.Counter(all_tokens)\n",
    "    unigram_model = {}\n",
    "    total_tokens = len(all_tokens)\n",
    "    for token, count in unigram_counts.items():\n",
    "        unigram_model[token] = (count + 1) / (total_tokens + vocab_size)\n",
    "    \n",
    "    unigram_path = os.path.join(models_dir, f\"{file_prefix}_{group_code}_unigrams.json\")\n",
    "    with open(unigram_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(unigram_model, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"[OK] Modelo de unigramas guardado en '{unigram_path}'\")\n",
    "    \n",
    "    # 2. Construye y guarda el modelo de Bigramas\n",
    "    bigram_counts = collections.Counter()\n",
    "    unigram_context_counts = collections.Counter()\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split()\n",
    "        for i in range(len(tokens) - 1):\n",
    "            bigram_counts[(tokens[i], tokens[i+1])] += 1\n",
    "            unigram_context_counts[tokens[i]] += 1\n",
    "    \n",
    "    bigram_model = {}\n",
    "    for (token1, token2), count in bigram_counts.items():\n",
    "        context_count = unigram_context_counts[token1]\n",
    "        probability = (count + 1) / (context_count + vocab_size)\n",
    "        bigram_model[f\"{token1} {token2}\"] = probability\n",
    "        \n",
    "    bigram_path = os.path.join(models_dir, f\"{file_prefix}_{group_code}_bigrams.json\")\n",
    "    with open(bigram_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(bigram_model, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"[OK] Modelo de bigramas guardado en '{bigram_path}'\")\n",
    "    \n",
    "    # 3. Construye y guarda el modelo de Trigramas\n",
    "    trigram_counts = collections.Counter()\n",
    "    bigram_context_counts = collections.Counter()\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split()\n",
    "        for i in range(len(tokens) - 2):\n",
    "            trigram_counts[(tokens[i], tokens[i+1], tokens[i+2])] += 1\n",
    "            bigram_context_counts[(tokens[i], tokens[i+1])] += 1\n",
    "            \n",
    "    trigram_model = {}\n",
    "    for (token1, token2, token3), count in trigram_counts.items():\n",
    "        context_count = bigram_context_counts.get((token1, token2), 0)\n",
    "        probability = (count + 1) / (context_count + vocab_size)\n",
    "        trigram_model[f\"{token1} {token2} {token3}\"] = probability\n",
    "\n",
    "    trigram_path = os.path.join(models_dir, f\"{file_prefix}_{group_code}_trigrams.json\")\n",
    "    with open(trigram_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(trigram_model, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"[OK] Modelo de trigramas guardado en '{trigram_path}'\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    group_code = \"my_group\"\n",
    "    \n",
    "    training_file_20N = os.path.join(\"tercer-punto\", f\"20N_{group_code}_training.txt\")\n",
    "    training_file_BAC = os.path.join(\"tercer-punto\", f\"BAC_{group_code}_training.txt\")\n",
    "    \n",
    "    # Construye y guarda los modelos\n",
    "    build_ngram_models(training_file_20N, group_code, \"20N\")\n",
    "    build_ngram_models(training_file_BAC, group_code, \"BAC\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b982de2",
   "metadata": {},
   "source": [
    "(15p) Using the test dataset, calculate the perplexity of each language model. Report the results obtained. If you experience variable overflow, use probabilities in log space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "e4a544a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Calculando la perplejidad ---\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/20N_my_group_unigrams.json'\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/20N_my_group_bigrams.json'\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/20N_my_group_trigrams.json'\n",
      "\n",
      "Resultados de Perplejidad para 20N (Corpus):\n",
      "  Unigramas: 1256.35\n",
      "  Bigramas:  3104.00\n",
      "  Trigramas: 15012.65\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/BAC_my_group_unigrams.json'\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/BAC_my_group_bigrams.json'\n",
      "[OK] Modelo cargado desde 'cuarto-punto/models/BAC_my_group_trigrams.json'\n",
      "\n",
      "Resultados de Perplejidad para BAC (Corpus):\n",
      "  Unigramas: 825.99\n",
      "  Bigramas:  1163.46\n",
      "  Trigramas: 12544.12\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------- Utilidades de carga/normalización ----------\n",
    "\n",
    "def read_ngram_model(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            model = json.load(f)\n",
    "        print(f\"[OK] Modelo cargado desde '{file_path}'\")\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] Archivo no encontrado: '{file_path}'\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] No se pudo cargar el modelo desde '{file_path}': {e}\")\n",
    "        return None\n",
    "\n",
    "def _normalize_ngram_keys(model_dict, n):\n",
    "    \"\"\"\n",
    "    Convierte llaves 'a b' / 'a b c' en tuplas ('a','b'[, 'c']) para O(1) consistente y evita splits repetidos.\n",
    "    Devuelve (dict_normalizado, seguidores_por_contexto)\n",
    "    seguidores_por_contexto:\n",
    "        - bigrama: dict prev -> #tipos_siguientes\n",
    "        - trigrama: dict (prev1,prev2) -> #tipos_siguientes\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    followers = defaultdict(int)\n",
    "    if n == 1:\n",
    "        # unigramas ya están por token -> p(token)\n",
    "        # no se necesita followers\n",
    "        for k, v in model_dict.items():\n",
    "            out[(k,)] = float(v)\n",
    "        return out, {}\n",
    "    elif n == 2:\n",
    "        next_types = defaultdict(set)\n",
    "        for k, v in model_dict.items():\n",
    "            a, b = k.split(' ', 1)\n",
    "            out[(a, b)] = float(v)\n",
    "            next_types[a].add(b)\n",
    "        followers = {a: len(bs) for a, bs in next_types.items()}\n",
    "        return out, followers\n",
    "    elif n == 3:\n",
    "        next_types = defaultdict(set)\n",
    "        for k, v in model_dict.items():\n",
    "            a, b, c = k.split(' ', 2)\n",
    "            out[(a, b, c)] = float(v)\n",
    "            next_types[(a, b)].add(c)\n",
    "        followers = {ctx: len(cs) for ctx, cs in next_types.items()}\n",
    "        return out, followers\n",
    "    else:\n",
    "        raise ValueError(\"n debe ser 1, 2 o 3\")\n",
    "\n",
    "# ---------- Perplejidad optimizada ----------\n",
    "\n",
    "def calculate_perplexity(\n",
    "    testing_file,\n",
    "    unigram_model, bigram_model, trigram_model,\n",
    "    vocab_size=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Calcula perplejidad usando:\n",
    "      - Accesos O(1) (tuplas como claves)\n",
    "      - Conteos de contexto precomputados (evita sum(...) por llave)\n",
    "      - Log-probabilidades para estabilidad\n",
    "      - Lectura streaming del archivo de prueba\n",
    "    \"\"\"\n",
    "    if any(m is None for m in (unigram_model, bigram_model, trigram_model)):\n",
    "        return None, None, None\n",
    "\n",
    "    # Normaliza llaves y precomputa seguidores/contexts\n",
    "    uni, _ = _normalize_ngram_keys(unigram_model, 1)\n",
    "    bi, followers_bi = _normalize_ngram_keys(bigram_model, 2)\n",
    "    tri, followers_tri = _normalize_ngram_keys(trigram_model, 3)\n",
    "\n",
    "    # Tamaño de vocabulario: por defecto = #unigramas distintos\n",
    "    if vocab_size is None:\n",
    "        vocab_size = len(uni)\n",
    "\n",
    "    # Precalcula denominadores de Laplace para velocidad\n",
    "    # Nota: tu fórmula original usa (#tipos_siguientes + V) como denominador de suavizado.\n",
    "    # Mantengo esa lógica para equivalencia funcional, pero ahora O(1).\n",
    "    laplace_uni_denom = (len(uni) + vocab_size)\n",
    "\n",
    "    total_log_u = 0.0\n",
    "    total_log_b = 0.0\n",
    "    total_log_t = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Lectura streaming (sin cargar todo a memoria)\n",
    "    try:\n",
    "        with open(testing_file, 'r', encoding='utf-8') as f:\n",
    "            for raw in f:\n",
    "                s = raw.strip()\n",
    "                if not s:\n",
    "                    continue\n",
    "                tokens = s.split()\n",
    "                n = len(tokens)\n",
    "                if n == 0:\n",
    "                    continue\n",
    "\n",
    "                total_tokens += n\n",
    "\n",
    "                # -------- Unigram --------\n",
    "                for w in tokens:\n",
    "                    p = uni.get((w,), 0.0)\n",
    "                    if p <= 0.0:\n",
    "                        p = 1.0 / laplace_uni_denom\n",
    "                    total_log_u += math.log(p)\n",
    "\n",
    "                # -------- Bigram --------\n",
    "                # P(w1) + Π P(w_i | w_{i-1})\n",
    "                # w1 como unigrama\n",
    "                p1 = uni.get((tokens[0],), 0.0)\n",
    "                if p1 <= 0.0:\n",
    "                    p1 = 1.0 / laplace_uni_denom\n",
    "                total_log_b += math.log(p1)\n",
    "\n",
    "                for i in range(n - 1):\n",
    "                    a, b_ = tokens[i], tokens[i + 1]\n",
    "                    p = bi.get((a, b_), 0.0)\n",
    "                    if p <= 0.0:\n",
    "                        context_types = followers_bi.get(a, 0)\n",
    "                        p = 1.0 / (context_types + vocab_size)\n",
    "                    total_log_b += math.log(p)\n",
    "\n",
    "                # -------- Trigram --------\n",
    "                # P(w1) * P(w2|w1) * Π P(w_i | w_{i-2}, w_{i-1})\n",
    "                # w1 como unigrama\n",
    "                p1 = uni.get((tokens[0],), 0.0)\n",
    "                if p1 <= 0.0:\n",
    "                    p1 = 1.0 / laplace_uni_denom\n",
    "                total_log_t += math.log(p1)\n",
    "\n",
    "                if n >= 2:\n",
    "                    p2 = bi.get((tokens[0], tokens[1]), 0.0)\n",
    "                    if p2 <= 0.0:\n",
    "                        ctx_types = followers_bi.get(tokens[0], 0)\n",
    "                        p2 = 1.0 / (ctx_types + vocab_size)\n",
    "                    total_log_t += math.log(p2)\n",
    "\n",
    "                for i in range(n - 2):\n",
    "                    a, b_, c = tokens[i], tokens[i + 1], tokens[i + 2]\n",
    "                    p = tri.get((a, b_, c), 0.0)\n",
    "                    if p <= 0.0:\n",
    "                        ctx_types = followers_tri.get((a, b_), 0)\n",
    "                        p = 1.0 / (ctx_types + vocab_size)\n",
    "                    total_log_t += math.log(p)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] No se pudo leer el archivo de prueba '{testing_file}': {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    if total_tokens == 0:\n",
    "        return float('inf'), float('inf'), float('inf')\n",
    "\n",
    "    ppl_u = math.exp(-total_log_u / total_tokens)\n",
    "    ppl_b = math.exp(-total_log_b / total_tokens)\n",
    "    ppl_t = math.exp(-total_log_t / total_tokens)\n",
    "    return ppl_u, ppl_b, ppl_t\n",
    "\n",
    "# ---------- Script principal ----------\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    group_code = \"my_group\"\n",
    "\n",
    "    training_file_20N = os.path.join(\"tercer-punto\", f\"20N_{group_code}_training.txt\")\n",
    "    testing_file_20N  = os.path.join(\"tercer-punto\", f\"20N_{group_code}_testing.txt\")\n",
    "    training_file_BAC = os.path.join(\"tercer-punto\", f\"BAC_{group_code}_training.txt\")\n",
    "    testing_file_BAC  = os.path.join(\"tercer-punto\", f\"BAC_{group_code}_testing.txt\")\n",
    "\n",
    "    print(\"\\n\\n--- Calculando la perplejidad ---\")\n",
    "\n",
    "    # Modelos 20N\n",
    "    unigrams_20N = read_ngram_model(os.path.join(\"cuarto-punto\", \"models\", f\"20N_{group_code}_unigrams.json\"))\n",
    "    bigrams_20N  = read_ngram_model(os.path.join(\"cuarto-punto\", \"models\", f\"20N_{group_code}_bigrams.json\"))\n",
    "    trigrams_20N = read_ngram_model(os.path.join(\"cuarto-punto\", \"models\", f\"20N_{group_code}_trigrams.json\"))\n",
    "\n",
    "    # vocab: usa #unigramas; evita re-leer el corpus completo\n",
    "    vocab_size_20N = len(unigrams_20N) if unigrams_20N else None\n",
    "\n",
    "    if all([unigrams_20N, bigrams_20N, trigrams_20N]):\n",
    "        pp_uni_20N, pp_bi_20N, pp_tri_20N = calculate_perplexity(\n",
    "            testing_file_20N,\n",
    "            unigrams_20N, bigrams_20N, trigrams_20N,\n",
    "            vocab_size=vocab_size_20N\n",
    "        )\n",
    "        print(f\"\\nResultados de Perplejidad para 20N (Corpus):\")\n",
    "        print(f\"  Unigramas: {pp_uni_20N:.2f}\")\n",
    "        print(f\"  Bigramas:  {pp_bi_20N:.2f}\")\n",
    "        print(f\"  Trigramas: {pp_tri_20N:.2f}\")\n",
    "\n",
    "    # Modelos BAC\n",
    "    unigrams_BAC = read_ngram_model(os.path.join(\"cuarto-punto\", \"models\", f\"BAC_{group_code}_unigrams.json\"))\n",
    "    bigrams_BAC  = read_ngram_model(os.path.join(\"cuarto-punto\", \"models\", f\"BAC_{group_code}_bigrams.json\"))\n",
    "    trigrams_BAC = read_ngram_model(os.path.join(\"cuarto-punto\", \"models\", f\"BAC_{group_code}_trigrams.json\"))\n",
    "\n",
    "    vocab_size_BAC = len(unigrams_BAC) if unigrams_BAC else None\n",
    "\n",
    "    if all([unigrams_BAC, bigrams_BAC, trigrams_BAC]):\n",
    "        pp_uni_BAC, pp_bi_BAC, pp_tri_BAC = calculate_perplexity(\n",
    "            testing_file_BAC,\n",
    "            unigrams_BAC, bigrams_BAC, trigrams_BAC,\n",
    "            vocab_size=vocab_size_BAC\n",
    "        )\n",
    "        print(f\"\\nResultados de Perplejidad para BAC (Corpus):\")\n",
    "        print(f\"  Unigramas: {pp_uni_BAC:.2f}\")\n",
    "        print(f\"  Bigramas:  {pp_bi_BAC:.2f}\")\n",
    "        print(f\"  Trigramas: {pp_tri_BAC:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebe989d",
   "metadata": {},
   "source": [
    "(15p) Using your best language model, build a method/function that automatically generates sentences by receiving the first word of a sentence as input. Take different tests and document them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a902576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colombia okay <s> and it world where flicks i have <s> it one <s> </s>\n",
      "Economía do <s> </s>\n",
      "Gobierno excellent i their one dat wash thunderbolts this hes i at gov i im rape taller your families endangered\n",
      "Bogotá to just NUM i there the today will <s> was day i youll nasty that it thinking willis some what okinawa <s> brain take can a to nap </s>\n"
     ]
    }
   ],
   "source": [
    "import json, math, random\n",
    "from typing import Dict, List, Tuple, Iterable\n",
    "\n",
    "# ---------------- Utilidades ----------------\n",
    "\n",
    "def load_unigram_model(path:str) -> Dict[str, float]:\n",
    "    \"\"\"Carga un JSON {token: prob} o {token: count}. Normaliza a probas.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = json.load(f)\n",
    "    # Si ya son probabilidades que suman ~1, úsalo; si parecen conteos, normaliza.\n",
    "    vals = list(raw.values())\n",
    "    s = sum(vals)\n",
    "    # Si hay NaNs o s==0, error\n",
    "    if not math.isfinite(s) or s <= 0:\n",
    "        raise ValueError(\"Modelo vacío o inválido\")\n",
    "    # Normaliza siempre (robusto ante floats que no sumen 1 exactamente)\n",
    "    return {tok: float(c)/s for tok, c in raw.items()}\n",
    "\n",
    "def _build_temperature_probs(p: Dict[str,float], temperature: float) -> Tuple[List[str], List[float]]:\n",
    "    \"\"\"Aplica temperatura y devuelve listas paralelas (vocab, cdf).\"\"\"\n",
    "    if temperature <= 0:\n",
    "        raise ValueError(\"temperature debe ser > 0\")\n",
    "    # p^ (1/T), renormaliza\n",
    "    powed = {t: (pi ** (1.0/temperature)) for t, pi in p.items() if pi > 0.0}\n",
    "    z = sum(powed.values())\n",
    "    vocab = []\n",
    "    cdf = []\n",
    "    acc = 0.0\n",
    "    for t, w in powed.items():\n",
    "        acc += w / z\n",
    "        vocab.append(t)\n",
    "        cdf.append(acc)\n",
    "    # Asegura que el último sea 1.0 exacto\n",
    "    cdf[-1] = 1.0\n",
    "    return vocab, cdf\n",
    "\n",
    "def _sample_from_cdf(vocab: List[str], cdf: List[float]) -> str:\n",
    "    r = random.random()\n",
    "    # búsqueda lineal (rápida en práctica). Cambia a bisect si tu vocab es enorme.\n",
    "    for t, c in zip(vocab, cdf):\n",
    "        if r <= c:\n",
    "            return t\n",
    "    return vocab[-1]\n",
    "\n",
    "def _should_stop(token: str, stop_tokens: Iterable[str]) -> bool:\n",
    "    return token in stop_tokens or any(token.endswith(st) for st in stop_tokens)\n",
    "\n",
    "# ---------------- Generador ----------------\n",
    "\n",
    "class UnigramSentenceGenerator:\n",
    "    def __init__(self, unigram_probs: Dict[str,float], temperature: float = 1.0,\n",
    "                 stop_tokens: Iterable[str] = (\".\", \"!\", \"?\", \"</s>\")):\n",
    "        self.vocab, self.cdf = _build_temperature_probs(unigram_probs, temperature)\n",
    "        self.stop_tokens = set(stop_tokens)\n",
    "\n",
    "    def generate(self, first_word: str, max_len: int = 30, seed: int = None) -> str:\n",
    "        \"\"\"\n",
    "        Genera una oración que inicia con first_word y luego muestrea unigramas.\n",
    "        No re-tokeniza; asume que los tokens del modelo son por palabra.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        tokens = [first_word]\n",
    "        # Si el primer token ya es stop, devuelve inmediato\n",
    "        if _should_stop(first_word, self.stop_tokens):\n",
    "            return first_word\n",
    "        # Completa hasta max_len o hasta stop\n",
    "        for _ in range(max_len - 1):\n",
    "            nxt = _sample_from_cdf(self.vocab, self.cdf)\n",
    "            tokens.append(nxt)\n",
    "            if _should_stop(nxt, self.stop_tokens):\n",
    "                break\n",
    "        # Pegado simple (si tu corpus maneja signos como tokens separados, esto es suficiente)\n",
    "        sent = \" \".join(tokens)\n",
    "        # Arreglo menor de espacios antes de puntuación común\n",
    "        sent = sent.replace(\" .\", \".\").replace(\" ,\", \",\").replace(\" !\", \"!\").replace(\" ?\", \"?\")\n",
    "        return sent\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1) Carga tu mejor modelo: BAC_unigramas\n",
    "    uni = load_unigram_model(\"cuarto-punto/models/BAC_my_group_unigrams.json\")\n",
    "\n",
    "    # 2) Crea el generador (puedes jugar con la temperatura)\n",
    "    gen_cool = UnigramSentenceGenerator(uni, temperature=0.8)  # más conservador\n",
    "    gen_neutral = UnigramSentenceGenerator(uni, temperature=1.0)\n",
    "    gen_hot = UnigramSentenceGenerator(uni, temperature=1.3)   # más diverso\n",
    "\n",
    "    # 3) Pruebas\n",
    "    print(gen_neutral.generate(\"Colombia\", max_len=20, seed=42))\n",
    "    print(gen_cool.generate(\"Economía\", max_len=20, seed=42))\n",
    "    print(gen_hot.generate(\"Gobierno\", max_len=20, seed=42))\n",
    "    print(gen_neutral.generate(\"Bogotá\", max_len=30, seed=7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
